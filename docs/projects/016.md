---
nav_order: 80
parent: í”„ë¡œì íŠ¸
title: '[DONE] LSTM+Attention ì•„í‚¤í…ì²˜ ê°œì„  - ì„±ëŠ¥ ì €í•˜ ì›ì¸ í•´ê²°'
---

# LSTM+Attention ì•„í‚¤í…ì²˜ ê°œì„  - ì„±ëŠ¥ ì €í•˜ ì›ì¸ í•´ê²°
{:.no_toc}

## ëª©ì°¨
{:.no_toc}

1. TOC
{:toc}

---

| ìƒíƒœ | ìƒì„±ì¼ | ìˆ˜ì •ì¼ |
|------|--------|--------|
| completed | 2025-12-24 | 2025-12-25 |

---

## Goal
009ì—ì„œ êµ¬í˜„í•œ LSTM+Attention ëª¨ë¸ì´ ê¸°ì¡´ LSTM ëŒ€ë¹„ ì„±ëŠ¥ì´ ì €í•˜ë˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ , Attentionì˜ ì´ì ì„ ì œëŒ€ë¡œ í™œìš©í•  ìˆ˜ ìˆë„ë¡ ì•„í‚¤í…ì²˜ë¥¼ ê°œì„ í•œë‹¤.

## Background

### í˜„ì¬ ì„±ëŠ¥ ë¹„êµ: LSTM vs Bahdanau vs Multi-head Attention

#### LONG GATE

| Metric | LSTM | Bahdanau | Multi-head | Best |
|--------|------|----------|------------|------|
| Label ratio | 17.57% | 17.57% | 17.57% | - |
| PR-AUC | 0.2026 | 0.1941 | **0.2260** | ğŸ† Multi-head |
| Coverage | 12.79% | 9.36% | 14.46% | - |
| Precision | 21.21% | 17.72% | **23.55%** | ğŸ† Multi-head |
| Recall | 15.44% | 9.44% | **19.38%** | ğŸ† Multi-head |
| **Precision@1%** | 15.86% | 4.14% | **44.14%** | ğŸ† Multi-head |
| Precision@5% | 19.62% | 15.91% | **29.49%** | ğŸ† Multi-head |
| Precision@10% | 20.56% | 17.68% | **23.37%** | ğŸ† Multi-head |
| Avg PnL | -0.2652% | -0.6036% | **-0.2225%** | ğŸ† Multi-head |
| Hit Rate | 45.63% | 41.65% | **46.49%** | ğŸ† Multi-head |
| **PnL@1%** | -0.1409% | -0.9715% | **+0.4865%** | ğŸ† Multi-head |
| PnL@5% | -0.3371% | -1.0017% | **-0.1290%** | ğŸ† Multi-head |
| PnL@10% | **-0.2663%** | -0.6068% | -0.3523% | LSTM |

#### SHORT GATE

| Metric | LSTM | Bahdanau | Multi-head | Best |
|--------|------|----------|------------|------|
| Label ratio | 19.39% | 19.39% | 19.39% | - |
| PR-AUC | **0.2355** | 0.2049 | 0.2011 | ğŸ† LSTM |
| Coverage | 15.53% | 23.14% | 15.00% | - |
| Precision | **24.54%** | 22.80% | 20.92% | ğŸ† LSTM |
| Recall | 19.65% | **27.22%** | 16.19% | Bahdanau |
| **Precision@1%** | **36.55%** | 18.62% | 8.97% | ğŸ† LSTM |
| Precision@5% | **31.41%** | 16.32% | 14.81% | ğŸ† LSTM |
| Precision@10% | **25.43%** | 21.45% | 17.68% | ğŸ† LSTM |
| Avg PnL | 0.1461% | **0.1741%** | 0.0507% | Bahdanau |
| Hit Rate | 54.24% | **54.66%** | 52.99% | Bahdanau |
| **PnL@1%** | **+0.5956%** | +0.1426% | -0.2924% | ğŸ† LSTM |
| PnL@5% | **+0.3440%** | +0.0050% | -0.0981% | ğŸ† LSTM |
| PnL@10% | **+0.1855%** | +0.1282% | +0.0017% | ğŸ† LSTM |

### í•µì‹¬ ë°œê²¬ ìš”ì•½

#### 1. Multi-head Attentionì˜ ê·¹ë‹¨ì  ë¹„ëŒ€ì¹­
| Gate | ê²°ê³¼ | Precision@1% ë³€í™” | PnL@1% ë³€í™” |
|------|------|-------------------|-------------|
| **LONG** | ğŸ† **Baseline ëŒ€í­ ì´ˆê³¼** | 15.86% â†’ **44.14%** (+28.3%p) | -0.14% â†’ **+0.49%** |
| **SHORT** | âŒ **Baseline ëŒ€í­ í•˜íšŒ** | 36.55% â†’ **8.97%** (-27.6%p) | +0.60% â†’ **-0.29%** |

#### 2. Bahdanau vs Multi-head ë¹„êµ
| êµ¬ë¶„    | Bahdanau         | Multi-head              |
| ----- | ---------------- | ----------------------- |
| LONG  | âŒ Baselineë³´ë‹¤ ë‚˜ì¨  | ğŸ† Baseline í¬ê²Œ ì´ˆê³¼       |
| SHORT | âŒ Baselineë³´ë‹¤ ë‚˜ì¨  | âŒâŒ Baselineë³´ë‹¤ **ë”** ë‚˜ì¨  |
| íŒ¨í„´    | ì–‘ìª½ ë‹¤ ë‚˜ì¨ (ê· ì¼í•œ ì‹¤íŒ¨) | LONG íŠ¹í™”, SHORT ì‹¤íŒ¨ (ë¹„ëŒ€ì¹­) |

#### 3. ë¬¸ì œ ì¬ì •ì˜ (ìˆ˜ì •ë¨)

**1ë²ˆ ì›ì¸ì˜ í™•ì¥:**
- **1a (Bahdanau íŠ¹í™”)**: last_hiddenì„ ë²„ë¦¬ê³  contextë§Œ ì‚¬ìš© â†’ **ê°€ì¥ ëª…í™•í•œ ì›ì¸-í•´ê²° ë§¤ì¹­**
- **1b (Multi-head í™•ì¥)**: GlobalAveragePoolingìœ¼ë¡œ ìš”ì•½ ë°©ì‹ ë³€ê²½ â†’ **SHORT í¬ì„ ê°€ëŠ¥ì„±**
- Multi-headë„ **í™•ì¥ëœ 1ë²ˆ**ì— í•´ë‹¹ (baselineì˜ last_hidden ìš”ì•½ì„ ë²„ë¦¼)

**5ë²ˆ (LONG/SHORT ë¹„ëŒ€ì¹­)ì˜ ê²©í•˜:**
- "ê²°ê³¼ê°€ ë¹„ëŒ€ì¹­"ì´ë¼ëŠ” ê´€ì°° ì¤‘ì‹¬ â†’ **ê²€ì¦ í•„ìš”í•œ ê°€ì„¤**
- ëŒ€ì•ˆ ì„¤ëª…ë“¤ ì¡´ì¬: Gradient conflict, GlobalAvgPooling í¬ì„, Checkpoint ê¸°ì¤€ ë¶ˆì¼ì¹˜
- 1ë²ˆ í•´ê²° í›„ ì¬í‰ê°€ í•„ìš”

#### 4. ì‹œì‚¬ì 

1. **Attention êµ¬ì¡° ìì²´ëŠ” íš¨ê³¼ê°€ ìˆìŒ** (Multi-head LONG ê²°ê³¼ë¡œ ì¦ëª…)
2. **1ë²ˆì´ ìµœìš°ì„  í•´ê²° ëŒ€ìƒ** - Bahdanau(1a)ì™€ Multi-head(1b) ëª¨ë‘ í•´ë‹¹
3. **5ë²ˆì€ ê°€ì„¤ ë‹¨ê³„** - 1ë²ˆ, ëŒ€ì•ˆ ê°€ì„¤(D: checkpoint ê¸°ì¤€) ê²€ì¦ í›„ íŒë‹¨
4. **Quick Win**: Multi-head LONG + LSTM SHORT ì•™ìƒë¸”ë¡œ ì¦‰ì‹œ ì–‘ìª½ best ë‹¬ì„± ê°€ëŠ¥

## ì›ì¸ ë¶„ì„

### ì›ì¸ë³„ ì¤‘ìš”ë„ ìš”ì•½ (Multi-head ê²°ê³¼ ë°˜ì˜, ìˆ˜ì •ë¨)

| ì›ì¸ | ì¤‘ìš”ë„ | Bahdanau | Multi-head | ë¹„ê³  |
|------|--------|----------|------------|------|
| **1a) last_hidden ë²„ë¦¼** (íŠ¹ì •) | â­â­â­ ìµœìƒ | âœ… **í•µì‹¬** | âŒ í•´ë‹¹ì—†ìŒ | Bahdanau íŠ¹í™” ë¬¸ì œ |
| **1b) ìš”ì•½ ë°©ì‹ ë³€ê²½** (ì¼ë°˜) | â­â­â­ ìµœìƒ | âœ… í•´ë‹¹ | âœ… **í•´ë‹¹** | GlobalAvgPoolingë„ ìš”ì•½ ë°©ì‹ ë³€ê²½ |
| **2) ìŠ¤í¼ë¦¬ì–´ìŠ¤ íŒ¨í„´** | â­â­ ìƒ | âœ… ê°€ëŠ¥ | âœ… ê°€ëŠ¥ | 2ì°¨ì  ë¬¸ì œ |
| **3) Query ì„¤ê³„ ë¶€ì í•©** | â­ ì¤‘ | âœ… í•´ë‹¹ | âŒ í•´ë‹¹ì—†ìŒ | Multi-headëŠ” Self-attention |
| **4) íŒŒë¼ë¯¸í„°+ì •ê·œí™”** | â­â­ ì¤‘~ìƒ | âœ… í•´ë‹¹ | âœ… í•´ë‹¹ | ë³´ì¡° ìš”ì¸ |
| **5) LONG/SHORT ë¹„ëŒ€ì¹­** | â­â­ ìƒ (**ê²€ì¦ í•„ìš”**) | â–³ ì¼ë¶€ | â–³ ê°€ëŠ¥ | ê°€ì„¤ ë‹¨ê³„, ëŒ€ì•ˆ ì„¤ëª… ì¡´ì¬ |

### 1) ìš”ì•½ ë°©ì‹ ë³€ê²½ìœ¼ë¡œ baseline ì¥ì  ìƒì‹¤ (ì¤‘ìš”ë„: â­â­â­ ìµœìƒ)

> **í•µì‹¬**: Attention ì¶”ê°€ê°€ ì•„ë‹ˆë¼, **baselineì´ ì“°ë˜ 'ë§ˆì§€ë§‰ ìƒíƒœ ìš”ì•½'ì„ ë²„ë¦¬ê³  ë‹¤ë¥¸ ìš”ì•½ì„ ì‚¬ìš©**í•œ ê²ƒì´ ë¬¸ì œ

#### 1a) Bahdanau íŠ¹í™” ë¬¸ì œ: last_hidden ë²„ë¦¼ (í™•ë¥  70%)

```python
# Baseline LSTM
lstm_output = LSTM(units, return_sequences=False)(x)  # ë§ˆì§€ë§‰ hiddenë§Œ ì‚¬ìš©

# Bahdanau Attention êµ¬í˜„
base_lstm = LSTM(units, return_sequences=True)(x)     # ì „ì²´ hidden
last_hidden = base_lstm[:, -1, :]                     # queryë¡œë§Œ ì‚¬ìš©
context = attention(last_hidden, base_lstm)           # contextë§Œ ì¶œë ¥, last_hidden ë²„ë¦¼!
```

- **contextë§Œ ìµœì¢… ì¶œë ¥**ìœ¼ë¡œ ì“°ê³  last_hiddenì„ ë²„ë¦¼
- "attention ì¶”ê°€"ê°€ ì•„ë‹ˆë¼ **baselineì„ ëŒ€ì²´**í•´ë²„ë¦° êµ¬ì¡°
- Top-K ì„±ëŠ¥ ë¶•ê´´ì™€ ì¸ê³¼ ì—°ê²°ì´ ê¹”ë”í•¨

**Quick Win í•´ê²°ì±…:**
```python
lstm_output = Concatenate()([last_hidden, context])  # ì›ì¸-í•´ê²° ë§¤ì¹­ì´ ê°€ì¥ ëª…í™•
```

#### 1b) Multi-head í™•ì¥ ë¬¸ì œ: GlobalAveragePoolingìœ¼ë¡œ ìš”ì•½ ë³€ê²½ (í™•ë¥  50%)

```python
# Multi-head Attention êµ¬í˜„
base_lstm = LSTM(units, return_sequences=True)(x)     # ì „ì²´ hidden
attn_output = MultiHeadAttention(...)(base_lstm, base_lstm)  # Self-attention
lstm_output = GlobalAveragePooling1D()(attn_output)   # í‰ê· ìœ¼ë¡œ ìš”ì•½ â† ë¬¸ì œ!
```

- **last_hiddenì´ ì•„ë‹Œ GlobalAveragePooling**ìœ¼ë¡œ ìš”ì•½
- í‰ê·  í’€ë§ì´ **SHORT ì‹ í˜¸ë¥¼ í¬ì„**ì‹œí‚¬ ìˆ˜ ìˆìŒ (SHORTì€ íŠ¹ì • timestepì— ì§‘ì¤‘ëœ ì‹ í˜¸ì¼ ê°€ëŠ¥ì„±)
- Multi-headë„ **í™•ì¥ëœ 1ë²ˆ**ì— í•´ë‹¹

**í…ŒìŠ¤íŠ¸ ì‹¤í—˜:**
```python
# A: last_hidden + pooled ê²°í•©
lstm_output = Concatenate()([base_lstm[:, -1, :], GlobalAveragePooling1D()(attn_output)])

# B: í‰ê·  ëŒ€ì‹  last timestep ì‚¬ìš©
lstm_output = attn_output[:, -1, :]

# C: Attention pooling (í•™ìŠµ ê°€ëŠ¥í•œ ê°€ì¤‘í•©)
lstm_output = AttentionPooling()(attn_output)  # í‰ê· ì´ í¬ì„ì‹œí‚¤ëŠ” ì‹ í˜¸ë¥¼ ì‚´ë¦¼
```

**ì™¸ë¶€ ìë£Œ ê·¼ê±°:**
- [Jeremy Jordan - Understanding Attention](https://www.jeremyjordan.me/attention/): "context vectorë¥¼ hidden stateì™€ **concatenate**í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì "
- [Analytics Vidhya - Attention Mechanism](https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/): BahdanauëŠ” last_hiddenì„ **ë³´ì™„**í•˜ê¸° ìœ„í•´ attentionì„ ì¶”ê°€í•œ ê²ƒ (ëŒ€ì²´ê°€ ì•„ë‹˜)

**ê²°ë¡ **: **1ë²ˆì´ Bahdanauì™€ Multi-head ëª¨ë‘ì˜ ìµœìš°ì„  ì›ì¸/í•´ê²°ì±…**. BahdanauëŠ” 1a(concat), Multi-headëŠ” 1b(pooling ablation)ë¡œ ì ‘ê·¼.

### 2) Attentionì´ ìŠ¤í¼ë¦¬ì–´ìŠ¤ íŒ¨í„´ í•™ìŠµ (ì¤‘ìš”ë„: â­â­ ìƒ, í™•ë¥  50%)

**í˜„ì¬ ë¬¸ì œ:**
- ê²Œì´íŠ¸ ë¼ë²¨ì´ `upside/downside ratio + min return` ì¡°ê±´ ê¸°ë°˜ â†’ ë¼ë²¨ ë…¸ì´ì¦ˆ ì¡´ì¬
- Attentionì€ timestepë³„ ì¤‘ìš”ë„ë¥¼ í•™ìŠµí•˜ë¯€ë¡œ, ìš°ì—°íˆ ë§ì€ ìƒ˜í”Œì˜ íŠ¹ì • êµ¬ê°„ì„ "ì¤‘ìš” ì‹ í˜¸"ë¡œ ì˜ëª» í•™ìŠµ
- í˜„ì¬ êµ¬í˜„ì— **attention weightì— ëŒ€í•œ ì •ê·œí™” ì—†ìŒ** (dropout, entropy reg ë“±)
- LONGì—ì„œ "í™•ì‹ (top 1%)ì¼ìˆ˜ë¡ ë” ë‚˜ì¨" íŒ¨í„´ â†’ ìŠ¤í¼ë¦¬ì–´ìŠ¤ í•™ìŠµê³¼ ì¼ì¹˜

**ì™¸ë¶€ ìë£Œ ê·¼ê±°:**
- [arxiv - Your Attention Matters](https://arxiv.org/html/2507.20453): "attention mechanismsì˜ **noiseì™€ spurious correlationsì— ëŒ€í•œ robustnessëŠ” ì˜ ì—°êµ¬ë˜ì§€ ì•Šì•˜ìŒ**. Softmax attentionì´ spurious correlationì— ì·¨ì•½"
- [ScienceDirect - Rethinking Attention in TSC](https://www.sciencedirect.com/science/article/abs/pii/S0020025523000968): "FMLAëŠ” **mask mechanismìœ¼ë¡œ time seriesì˜ noise interferenceë¥¼ ì¤„ì„**. Noiseë¡œ ì¸í•œ fluctuationì´ local pattern recognition ì„±ëŠ¥ì„ ì•…í™”"
- [arxiv - Complexity Matters](https://arxiv.org/html/2403.03375): "neural networksëŠ” inherently **spurious featuresë¥¼ í•™ìŠµí•˜ëŠ” ê²½í–¥**. Training ì„±ëŠ¥ì€ ì¢‹ì§€ë§Œ distribution shiftì— ì·¨ì•½í•œ ì´ìœ "

**ê²°ë¡ **: Attentionì´ spurious correlationì— ì·¨ì•½í•˜ë‹¤ëŠ” ì—°êµ¬ ê²°ê³¼ ì¡´ì¬. ë‹¨, 1ë²ˆ ë¬¸ì œê°€ í•´ê²°ëœ í›„ì—ë„ ë°œìƒí•  ìˆ˜ ìˆëŠ” **2ì°¨ì  ë¬¸ì œ**.

**ì²´í¬ í¬ì¸íŠ¸:**
- Trainì—ì„œëŠ” ì¢‹ì•„ì§€ëŠ”ë° Val/Test PR-AUC, top-k PnLì´ ì•…í™”ë˜ë©´ ê³¼ì í•©/ìŠ¤í¼ë¦¬ì–´ìŠ¤ ê°€ëŠ¥ì„±ì´ í¼

**ê°œì„  ë°©ì•ˆ:**
- Attention dropout ì¶”ê°€
- Attention entropy regularization ê²€í† 
- Train vs Val ì„±ëŠ¥ ì°¨ì´ ëª¨ë‹ˆí„°ë§ìœ¼ë¡œ ê³¼ì í•© ê°ì§€

### 3) Bahdanau query ì„¤ê³„ê°€ ë¶„ë¥˜ì— ë¶€ì í•© (ì¤‘ìš”ë„: â­ ì¤‘, í™•ë¥  30%)

**í˜„ì¬ ë¬¸ì œ:**
- `query = last_hidden`ìœ¼ë¡œ ê³ ì •
- ë¶„ë¥˜/ê²Œì´íŒ…ì€ NMT ì •ë ¬ ë¬¸ì œì™€ ë‹¤ë¦„
- last_hiddenì´ ë¶ˆì•ˆì •í•˜ë©´ attentionì´ "ì—‰ëš±í•œ timestep" ì„ íƒ
- ë˜ëŠ” last_hiddenì´ ì´ë¯¸ ìš”ì•½ì¸ë°, ë‹¤ì‹œ valuesë¥¼ ì„ íƒí•´ ìš”ì•½í•˜ë©´ì„œ ì •ë³´ í¬ì„

**ì™¸ë¶€ ìë£Œ ê·¼ê±°:**
- [D2L - Bahdanau Attention](https://d2l.ai/chapter_attention-mechanisms-and-transformers/bahdanau-attention.html): BahdanauëŠ” **seq2seq (ë²ˆì—­) íƒœìŠ¤í¬ë¥¼ ìœ„í•´ ì„¤ê³„**ë¨
- [MachineLearningMastery - Bahdanau Attention](https://machinelearningmastery.com/the-bahdanau-attention-mechanism/): "RNN encoder-decoder with Bahdanau attention treats **decoder hidden state at previous time step as query**"

**ì™œ "ê·¼ë³¸ ì›ì¸"ì´ ì•„ë‹Œ "ìµœì í™” ë¬¸ì œ"ì¸ê°€:**

1. **BahdanauëŠ” seq2seqìš©ì´ì§€ë§Œ, ë¶„ë¥˜ì— ì ìš© ë¶ˆê°€ëŠ¥í•œ ê²ƒì€ ì•„ë‹˜**
   - seq2seqì—ì„œëŠ” decoderê°€ step-by-stepìœ¼ë¡œ ì¶œë ¥ì„ ìƒì„±í•˜ë¯€ë¡œ "ì´ì „ decoder hidden"ì„ queryë¡œ ì‚¬ìš©
   - ë¶„ë¥˜ì—ì„œëŠ” decoderê°€ ì—†ìœ¼ë¯€ë¡œ "encoderì˜ last_hidden"ì„ queryë¡œ ëŒ€ì²´í•˜ëŠ” ê²ƒì€ í•©ë¦¬ì ì¸ adaptation
   - ì´ adaptation ìì²´ê°€ í‹€ë¦° ê²ƒì€ ì•„ë‹ˆë©°, ë§ì€ ë…¼ë¬¸ì—ì„œ ì´ëŸ° ë°©ì‹ìœ¼ë¡œ ë¶„ë¥˜ì— Bahdanauë¥¼ ì ìš©

2. **Query ì„ íƒë³´ë‹¤ ì¶œë ¥ ê²°í•© ë°©ì‹ì´ ë” ì¤‘ìš”**
   - Queryê°€ last_hiddenì´ë“  trainable vectorì´ë“ , ìµœì¢… ì¶œë ¥ì„ ì–´ë–»ê²Œ ì‚¬ìš©í•˜ëŠëƒê°€ ë” ê²°ì •ì 
   - í˜„ì¬ ë¬¸ì œì˜ í•µì‹¬ì€ query ì„¤ê³„ê°€ ì•„ë‹ˆë¼ **contextë§Œ ì‚¬ìš©í•˜ê³  last_hiddenì„ ë²„ë¦° ê²ƒ** (1ë²ˆ ë¬¸ì œ)
   - 1ë²ˆì„ í•´ê²°í•˜ë©´ query ì„¤ê³„ì˜ ì˜í–¥ì€ ìƒëŒ€ì ìœ¼ë¡œ ì‘ì•„ì§

3. **Trainable context vectorëŠ” ì„±ëŠ¥ í–¥ìƒì˜ "ì˜µì…˜"ì´ì§€ "í•„ìˆ˜"ê°€ ì•„ë‹˜**
   - Trainable queryë¥¼ ì‚¬ìš©í•˜ë©´ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ê°€ ì¶”ê°€ë˜ì–´ ë” flexibleí•´ì§ˆ ìˆ˜ ìˆìŒ
   - ê·¸ëŸ¬ë‚˜ ì´ê²ƒì€ baselineì„ ë„˜ì–´ì„œê¸° ìœ„í•œ **ì¶”ê°€ ìµœì í™”**ì´ì§€, í˜„ì¬ ì„±ëŠ¥ ì €í•˜ì˜ ì›ì¸ì€ ì•„ë‹˜

**ê²°ë¡ **: Query ì„¤ê³„ ë¬¸ì œëŠ” ìµœì í™”/ê°œì„  ì—¬ì§€ì´ì§€, í˜„ì¬ ì„±ëŠ¥ ì €í•˜ì˜ ê·¼ë³¸ ì›ì¸ì€ ì•„ë‹˜. 1ë²ˆ í•´ê²° í›„ ì¶”ê°€ ì‹¤í—˜ìœ¼ë¡œ ê²€í† .

**ê°œì„  ë°©ì•ˆ (ì„ íƒì ):**
- Trainable context vector ê¸°ë°˜ attention pooling ê²€í† 
- Self-attention + Residual + LayerNorm ì•ˆì •í™”

### 4) íŒŒë¼ë¯¸í„° ì¦ê°€ + ì •ê·œí™” ë¶€ì¡± (ì¤‘ìš”ë„: â­â­ ì¤‘~ìƒ, í™•ë¥  40%)

**í˜„ì¬ ë¬¸ì œ:**
- Attention ì¶”ê°€ë¡œ íŒŒë¼ë¯¸í„° ì¦ê°€ (37K â†’ 46K/71K)
- Attention ë‚´ë¶€ì— dropout/LayerNorm/Residual ì—†ìŒ
- í•™ìŠµ metricì´ `val_loss`ì¸ë° ì‹¤ì œ ëª©í‘œëŠ” PR-AUC, top-k PnL

**ì™¸ë¶€ ìë£Œ ê·¼ê±°:**
- [ResearchGate - LSTMs vs Attention for Financial TS](https://www.researchgate.net/publication/329798567_A_Comparison_of_LSTMs_and_Attention_Mechanisms_for_Forecasting_Financial_Time_Series): "attention mechanism implementationsì—ì„œ 16 cellsë§Œ ì‚¬ìš©í•´ë„ ì•½ 10,000 parameters, ë°ì´í„° í¬ì¸íŠ¸ë³´ë‹¤ parametersê°€ ë§ìœ¼ë©´ **overfittingì€ í•„ì—°ì **"
- [Springer - DARVFL-LSTM](https://link.springer.com/article/10.1007/s11227-025-07360-1): "**Vanilla full attention mechanism (FAM)ì€ temporal redundancyì— ì·¨ì•½**í•˜ê³  irrelevant interferenceì— overfití•˜ê¸° ì‰¬ì›€"
- [MachineLearningMastery - LSTM Regularization](https://machinelearningmastery.com/use-dropout-lstm-networks-time-series-forecasting/): "LSTMì—ì„œ dropoutì€ **non-recurrent connectionsì—ë§Œ ì ìš©**í•´ì•¼ í•¨"
- [CopyProgramming - Preventing LSTM Overfitting](https://copyprogramming.com/howto/preventing-overfitting-of-lstm-on-small-dataset): "LSTMs are prone to overfitting due to large number of parameters"

**ê²°ë¡ **: íŒŒë¼ë¯¸í„° ì¦ê°€ê°€ overfittingì„ ìœ ë°œí•  ìˆ˜ ìˆë‹¤ëŠ” ì™¸ë¶€ ì¦ê±° ì¡´ì¬. ê·¸ëŸ¬ë‚˜ **14,593 samplesì´ë©´ 71K paramsë¥¼ í•™ìŠµí•˜ê¸°ì— ì¶©ë¶„**í•œ í¸ì´ë¯€ë¡œ, ì£¼ìš” ì›ì¸ë³´ë‹¤ëŠ” ë³´ì¡° ìš”ì¸ì¼ ê°€ëŠ¥ì„±.

**ê°œì„  ë°©ì•ˆ:**
- Learning rate ë‚®ì¶”ê¸° (1e-3 â†’ 1e-4~3e-4)
- Attention ì¶œë ¥ì— LayerNorm ì¶”ê°€
- Metricì„ `AUC(curve='PR')`ë¡œ ë³€ê²½

### 5) LONG/SHORT ë¹„ëŒ€ì¹­ (ì¤‘ìš”ë„: â­â­ ìƒ, **ê²€ì¦ í•„ìš”í•œ ê°€ì„¤**)

**Multi-head ê²°ê³¼ì—ì„œ ê´€ì°°ëœ í˜„ìƒ:**

```
Multi-head Attention ê²°ê³¼:
- LONG:  Precision@1% 44.14% (Baseline 15.86% ëŒ€ë¹„ +28.3%p) ğŸ†
- SHORT: Precision@1%  8.97% (Baseline 36.55% ëŒ€ë¹„ -27.6%p) âŒ
```

**ê°€ì„¤ A: LONG/SHORT íŒ¨í„´ ë¹„ëŒ€ì¹­**
- Multi-headì˜ 4ê°œ attention headsê°€ **LONG íŒ¨í„´ì— í¸í–¥**ë˜ì–´ í•™ìŠµ
- LONGê³¼ SHORTì˜ ì‹œê³„ì—´ íŒ¨í„´ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ

**âš ï¸ ì™œ "ìµœìƒ"ì´ ì•„ë‹Œ "ê²€ì¦ í•„ìš”í•œ ê°€ì„¤"ì¸ê°€:**

í˜„ì¬ ì¦ê±°ëŠ” **"ê²°ê³¼ê°€ ë¹„ëŒ€ì¹­"ì´ë¼ëŠ” ê´€ì°°** ì¤‘ì‹¬ì´ë©°, í•µì‹¬ ì›ì¸ìœ¼ë¡œ í™•ì •í•˜ê¸°ì—” ì´ë¦„.
Multi-head SHORT ë¶•ê´´ë¥¼ ì„¤ëª…í•˜ëŠ” **ëŒ€ì•ˆ ê°€ì„¤ë“¤**ì´ ì¡´ì¬:

| ëŒ€ì•ˆ ê°€ì„¤ | ì„¤ëª… | ê²€ì¦ ë°©ë²• |
|----------|------|----------|
| **B: Gradient Conflict** | ë©€í‹°íƒœìŠ¤í¬(LONG/SHORT) ê³µìœ  trunkë¡œ ì¸í•œ gradient ì¶©ëŒ | LONG/SHORT ë³„ë„ ëª¨ë¸ë¡œ ë¶„ë¦¬ í•™ìŠµ |
| **C: GlobalAvgPooling í¬ì„** | í‰ê·  í’€ë§ì´ SHORT ì‹ í˜¸(íŠ¹ì • timestep ì§‘ì¤‘)ë¥¼ í¬ì„ | Pooling ablation (avg â†’ last/max/attention) |
| **D: Checkpoint ì„ íƒ ê¸°ì¤€ ë¶ˆì¼ì¹˜** | val_loss/ROC AUC ê¸°ì¤€ì´ PR-AUC/top-k/PnL ëª©í‘œì™€ ë¶ˆì¼ì¹˜ | PR-AUC(curve='PR') ëª¨ë‹ˆí„°ë¡œ checkpoint ì¬ì„ íƒ |

**5ë²ˆì„ 'ê²€ì¦ëœ ì›ì¸'ìœ¼ë¡œ ë§Œë“¤ê¸° ìœ„í•œ ìµœì†Œ ì‹¤í—˜:**

1. **LONG/SHORT ë³„ë„ attention branch** â†’ ë¹„ëŒ€ì¹­ ê°€ì„¤ ì§ì ‘ ê²€ì¦
2. **Pooling ablation**: Avg â†’ last timestep / max / attention pooling â†’ í¬ì„ ê°€ì„¤ ê²€ì¦
3. **PR-AUC ëª¨ë‹ˆí„° checkpoint** â†’ ì„ íƒ ê¸°ì¤€ ë¶ˆì¼ì¹˜ ì œê±° í›„ ì¬í‰ê°€

**ì¦ê±° ì •ë¦¬:**
| ê´€ì°° | 5ë²ˆ ì§€ì§€ | ëŒ€ì•ˆ ì„¤ëª… |
|------|---------|----------|
| Multi-head LONG ì„±ê³µ | âœ… | Attention ìì²´ëŠ” íš¨ê³¼ì  (1bì™€ ë¬´ê´€) |
| Multi-head SHORT ì‹¤íŒ¨ | â–³ | 1b(AvgPooling í¬ì„), D(checkpoint ê¸°ì¤€) ê°€ëŠ¥ |
| Bahdanau ì–‘ìª½ ì‹¤íŒ¨ | âŒ | 1a(last_hidden ë²„ë¦¼)ë¡œ ì¶©ë¶„íˆ ì„¤ëª… |

**ê²°ë¡ **: 5ë²ˆì€ **ê°€ëŠ¥í•œ ê°€ì„¤**ì´ë‚˜, 1ë²ˆ(ìš”ì•½ ë°©ì‹ ë³€ê²½)ê³¼ ëŒ€ì•ˆ ê°€ì„¤ë“¤ì„ ë¨¼ì € ê²€ì¦í•œ í›„ íŒë‹¨í•´ì•¼ í•¨.

**ê°œì„  ë°©ì•ˆ (ê²€ì¦ í›„ ì ìš©):**
- **Option A**: LONG/SHORT ë³„ë„ attention layer
- **Option B**: LONG/SHORT ì™„ì „ ë³„ë„ ëª¨ë¸
- **Option C**: Task-specific attention heads í• ë‹¹

### ê¶Œì¥ í•´ê²° ìš°ì„ ìˆœìœ„ (ìˆ˜ì •ë¨)

**Bahdanau Attention:**
1. **1a í•´ê²° (Quick Win)**: `concat([last_hidden, context])` ì ìš©
2. **ì¬í‰ê°€ í›„** 2, 4ë²ˆ ê²€í† 

**Multi-head Attention:**
1. **1b í•´ê²° ìš°ì„ **: Pooling ablationìœ¼ë¡œ ìš”ì•½ ë°©ì‹ ì‹¤í—˜
   - A: `concat([last_hidden, GlobalAvgPooling(attn)])`
   - B: `attn[:, -1, :]` (last timestep)
   - C: AttentionPooling (í•™ìŠµ ê°€ëŠ¥í•œ ê°€ì¤‘í•©)
2. **D í•´ê²°**: PR-AUC ëª¨ë‹ˆí„°ë¡œ checkpoint ì¬ì„ íƒ
3. **1b + Dë¡œ ì•ˆ ë˜ë©´** 5ë²ˆ ê²€ì¦ ì‹¤í—˜ ì§„í–‰

**ì¢…í•© ì „ëµ:**
```
Step 1: Bahdanau 1a (concat) â†’ ì–‘ìª½ baseline íšŒë³µ ëª©í‘œ
Step 2: Multi-head 1b (pooling ablation) â†’ SHORT ê°œì„  ì‹œë„
Step 3: Multi-head D (PR-AUC checkpoint) â†’ ì„ íƒ ê¸°ì¤€ ì •ë ¬
Step 4: (ìœ„ ì‹¤íŒ¨ ì‹œ) 5ë²ˆ ê²€ì¦ â†’ ë³„ë„ ëª¨ë¸/attention ê²€í† 
```

**Quick Win (ë‹¹ì¥ í™œìš© ê°€ëŠ¥):**
```
Multi-head LONG + Baseline LSTM SHORT ì•™ìƒë¸”
â†’ ê²€ì¦ ì‹¤í—˜ ì—†ì´ ë°”ë¡œ ì–‘ìª½ best ë‹¬ì„± ê°€ëŠ¥
```

## Scope

### Phase 1: Quick Win - ì•™ìƒë¸” (ìµœìš°ì„ )
- [ ] **Multi-head LONG + Baseline LSTM SHORT ì•™ìƒë¸”** í‰ê°€
  - Multi-head ëª¨ë¸ì—ì„œ LONG gateë§Œ ì‚¬ìš©
  - Baseline LSTMì—ì„œ SHORT gateë§Œ ì‚¬ìš©
- [ ] ì•™ìƒë¸” ì„±ëŠ¥ ì¸¡ì • (ê²€ì¦ ì—†ì´ ë°”ë¡œ ì–‘ìª½ best ë‹¬ì„±)

### Phase 2: Bahdanau 1a ìˆ˜ì • (ì›ì¸-í•´ê²° ë§¤ì¹­ ê°€ì¥ ëª…í™•)
- [x] `concat([last_hidden, context])` ì ìš©
- [x] Attention ì¶œë ¥ì— LayerNorm ì¶”ê°€
- [ ] ìˆ˜ì •ëœ Bahdanau ì„±ëŠ¥ ì¬í‰ê°€ (ì–‘ìª½ baseline íšŒë³µ ëª©í‘œ)

### Phase 3: Multi-head 1b Pooling Ablation
- [x] **A**: `concat([last_hidden, GlobalAvgPooling(attn)])` - last_hidden ë³µì›
- [x] **B**: `attn[:, -1, :]` - í‰ê·  ëŒ€ì‹  last timestep
- [x] **C**: AttentionPooling - í•™ìŠµ ê°€ëŠ¥í•œ ê°€ì¤‘í•©
- [ ] ê° ë°©ì‹ë³„ LONG/SHORT ì„±ëŠ¥ ë¹„êµ

### Phase 4: Checkpoint ì„ íƒ ê¸°ì¤€ ì •ë ¬ (ëŒ€ì•ˆ ê°€ì„¤ D ê²€ì¦)
- [x] PR-AUC(curve='PR') ëª¨ë‹ˆí„°ë¡œ EarlyStopping/ModelCheckpoint
- [ ] val_loss â†’ val_pr_aucë¡œ ë³€ê²½ í›„ ì¬í•™ìŠµ
- [ ] ê¸°ì¡´ ëª¨ë¸ê³¼ ì„±ëŠ¥ ë¹„êµ

### Phase 5: 5ë²ˆ ê²€ì¦ ì‹¤í—˜ (ìœ„ ë‹¨ê³„ë¡œ í•´ê²° ì•ˆ ë  ê²½ìš°)
- [ ] LONG/SHORT ë³„ë„ attention branch
- [ ] LONG ì „ìš© / SHORT ì „ìš© ì™„ì „ ë¶„ë¦¬ ëª¨ë¸
- [ ] ë¶„ë¦¬ ëª¨ë¸ vs í†µí•© ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ

## Technical Details

### Bahdanau 1a ìˆ˜ì •: last_hidden + context ê²°í•©

```python
# Before (í˜„ì¬) - last_hidden ë²„ë¦¼
context, _ = attention(last_hidden, base_lstm)
lstm_output = context  # last_hidden ë²„ë¦¼!

# After (ê°œì„ ) - last_hidden + context ê²°í•©
context, _ = attention(last_hidden, base_lstm)
lstm_output = Concatenate()([last_hidden, context])  # í•µì‹¬: last_hidden ë³´ì¡´
lstm_output = LayerNorm()(lstm_output)               # ì•ˆì •í™”
```

### Multi-head 1b ìˆ˜ì •: Pooling Ablation

```python
# í˜„ì¬ êµ¬í˜„ - GlobalAveragePooling
attn_output = MultiHeadAttention(...)(base_lstm, base_lstm)
lstm_output = GlobalAveragePooling1D()(attn_output)  # í‰ê· ìœ¼ë¡œ í¬ì„

# Option A: last_hidden + pooled ê²°í•©
last_hidden = base_lstm[:, -1, :]
pooled = GlobalAveragePooling1D()(attn_output)
lstm_output = Concatenate()([last_hidden, pooled])

# Option B: í‰ê·  ëŒ€ì‹  last timestep
lstm_output = attn_output[:, -1, :]

# Option C: Attention Pooling (í•™ìŠµ ê°€ëŠ¥í•œ ê°€ì¤‘í•©)
class AttentionPooling(Layer):
    def __init__(self, units):
        super().__init__()
        self.attention = Dense(1, activation='softmax')

    def call(self, x):
        # x: (batch, seq_len, hidden)
        weights = self.attention(x)  # (batch, seq_len, 1)
        weights = tf.nn.softmax(weights, axis=1)
        return tf.reduce_sum(weights * x, axis=1)

lstm_output = AttentionPooling(units)(attn_output)
```

### í•™ìŠµ ì„¤ì • ë³€ê²½: PR-AUC ëª¨ë‹ˆí„°

```python
# Before
callbacks = [EarlyStopping(monitor='val_loss', ...)]

# After - PR-AUC ê¸°ì¤€ìœ¼ë¡œ checkpoint ì„ íƒ
model.compile(
    loss='binary_crossentropy',
    metrics=[tf.keras.metrics.AUC(curve='PR', name='pr_auc')]
)
callbacks = [
    EarlyStopping(monitor='val_pr_auc', mode='max', patience=10),
    ModelCheckpoint(monitor='val_pr_auc', mode='max', save_best_only=True)
]
```

## Acceptance Criteria

### í•„ìˆ˜ ì¡°ê±´ (Baseline LSTM ìˆ˜ì¤€ ë‹¬ì„±)

| Gate | Metric | Baseline | í˜„ì¬ Best | ëª©í‘œ |
|------|--------|----------|-----------|------|
| LONG | Precision@1% | 15.86% | 44.14% (Multi-head) âœ… | â‰¥ 15% |
| LONG | PnL@1% | -0.1409% | +0.4865% (Multi-head) âœ… | â‰¥ 0% |
| SHORT | Precision@1% | 36.55% | 36.55% (LSTM) | â‰¥ 30% |
| SHORT | PnL@1% | +0.5956% | +0.5956% (LSTM) | â‰¥ 0.3% |

### ì„±ê³µ ì¡°ê±´ (ì–‘ìª½ ëª¨ë‘ Baseline ì´ˆê³¼)

**ì•™ìƒë¸”/ë¶„ë¦¬ ëª¨ë¸ ëª©í‘œ:**
- [ ] LONG: Multi-head ìˆ˜ì¤€ ìœ ì§€ (Precision@1% â‰¥ 40%, PnL@1% â‰¥ +0.4%)
- [ ] SHORT: Baseline ìˆ˜ì¤€ ìœ ì§€ (Precision@1% â‰¥ 35%, PnL@1% â‰¥ +0.5%)
- [ ] ì–‘ìª½ ë™ì‹œì— baseline ì´ìƒ ë‹¬ì„±

### ì´ìƒì  ì¡°ê±´ (í†µí•© ëª¨ë¸ë¡œ ë‹¬ì„±)
- [ ] ë‹¨ì¼ ëª¨ë¸ì—ì„œ LONG/SHORT ëª¨ë‘ baseline ì´ˆê³¼
- [ ] ì•™ìƒë¸” ì—†ì´ í†µí•© ì•„í‚¤í…ì²˜ë¡œ í•´ê²°

## Related
- [009-lstm-attention-hybrid](009-lstm-attention-hybrid.md) - ì›ë³¸ Attention êµ¬í˜„ (ì„±ëŠ¥ ì €í•˜ ë°œìƒ)
- [003-3-long-short-gate-separation](003-3-long-short-gate-separation.md) - Long/Short Gate 2-head êµ¬ì¡°

## Related Files
- `src/prediction/prediction_model.py` - BahdanauAttention, create_model_lstm ìˆ˜ì •

---

## Implementation Results

### Changed Files
| File | Changes |
|------|---------|
| `src/prediction/prediction_model.py` | +177 -12 lines |

### Detailed Changes

**1. AttentionPooling ë ˆì´ì–´ í´ë˜ìŠ¤ ì¶”ê°€**
- í•™ìŠµ ê°€ëŠ¥í•œ ê°€ì¤‘í•© í’€ë§ ë ˆì´ì–´
- GlobalAveragePooling1D ëŒ€ì•ˆìœ¼ë¡œ ì§‘ì¤‘ëœ ì‹ í˜¸ ë³´ì¡´
- Dense(tanh) â†’ Dense(1) â†’ Softmax â†’ Weighted Sum êµ¬ì¡°

**2. Bahdanau Attention ì•„í‚¤í…ì²˜ ìˆ˜ì • (Phase 2: 1a fix)**
- `concat_hidden` íŒŒë¼ë¯¸í„° ì¶”ê°€ (default: True)
- `Concatenate([last_hidden, context])` + `LayerNormalization` ì ìš©
- ê¸°ì¡´: contextë§Œ ì‚¬ìš© â†’ ìˆ˜ì •: last_hidden ë³´ì¡´

**3. Multi-head Attention Pooling Ablation (Phase 3: 1b fix)**
- `pooling_type` íŒŒë¼ë¯¸í„° ì¶”ê°€ (4ê°€ì§€ ì˜µì…˜):
  - `'avg'`: ì›ë˜ GlobalAveragePooling1D
  - `'last'`: ë§ˆì§€ë§‰ timestepë§Œ ì‚¬ìš©
  - `'attention'`: AttentionPooling (í•™ìŠµ ê°€ëŠ¥)
  - `'concat'`: last_hidden + avg pooled ê²°í•© (default)

**4. Checkpoint ì„ íƒ ê¸°ì¤€ ë³€ê²½ (Phase 4)**
- `CHECKPOINT_MONITOR`, `CHECKPOINT_MODE` ìƒìˆ˜ ì¶”ê°€
- model.compile()ì— PR-AUC ë©”íŠ¸ë¦­ ì¶”ê°€
- EarlyStopping/ModelCheckpointì— mode ì„¤ì • ê°€ëŠ¥

### Test Results
```
=== Test 1: Standard LSTM ===
Model name: lstm_gate
Total params: 28,866

=== Test 2: Bahdanau with concat (1a fix) ===
Model name: lstm_bahdanau_concat_gate
Total params: 41,474

=== Test 3: Bahdanau without concat (original) ===
Model name: lstm_bahdanau_gate
Total params: 37,122

=== Test 4: Multi-head concat pooling (1b fix) ===
Model name: lstm_multihead_concat_gate
Total params: 66,434

=== Test 5: Multi-head avg pooling (original) ===
Model name: lstm_multihead_gate
Total params: 62,082

=== Test 6: Multi-head last timestep ===
Model name: lstm_multihead_last_gate
Total params: 62,082

=== Test 7: Multi-head attention pooling ===
Model name: lstm_multihead_attnpool_gate
Total params: 66,306

=== All tests passed! ===
```

### Lessons Learned / Issues
- Bahdanau concat_hidden=Trueë¡œ ì•½ 4,352 params ì¦ê°€ (37K â†’ 41K)
- Multi-head concat poolingìœ¼ë¡œ ì•½ 4,352 params ì¦ê°€ (62K â†’ 66K)
- AttentionPoolingì€ attention í’€ë§ê³¼ ë¹„ìŠ·í•œ íŒŒë¼ë¯¸í„° ìˆ˜ (66K)

### Follow-up
- [x] ì‹¤ì œ í•™ìŠµ ì‹¤í—˜ìœ¼ë¡œ ì„±ëŠ¥ ë¹„êµ í•„ìš” (Phase 1 ì•™ìƒë¸” í¬í•¨)
- [ ] PR-AUC ëª¨ë‹ˆí„°ë¡œ checkpoint ì„ íƒ ì‹œ ì„±ëŠ¥ ë³€í™” ì¸¡ì •
- [ ] 5ë²ˆ ê²€ì¦ ì‹¤í—˜ (LONG/SHORT ë³„ë„ ëª¨ë¸) - í•„ìš” ì‹œ

### Pull Request
https://github.com/minhopark1271/trading/pull/12

### Completed
2025-12-25

---

## ì‹¤í—˜ ê²°ê³¼ ë¶„ì„ (2025-12-25)

> âš ï¸ **ì£¼ì˜**: ë‹¨ì¼ í•™ìŠµ ê²°ê³¼ì´ë¯€ë¡œ í™•ì •ì  ê²°ë¡ ì€ ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ. ëœë¤ ì‹œë“œ/ì´ˆê¸°í™”ì— ë”°ë¼ ë‹¤ë¥¸ íŒ¨í„´ ê°€ëŠ¥.

### í•µì‹¬ ê²°ë¡ : 016 ìˆ˜ì •ì˜ íš¨ê³¼ ì œí•œì 

| 016 ëª©í‘œ | ê²°ê³¼ | íŒì • |
|----------|------|------|
| Bahdanau LONG/SHORT ëª¨ë‘ baseline íšŒë³µ | LONG ì—¬ì „íˆ baselineë³´ë‹¤ ë‚˜ì¨ (20.69% vs 23.45%) | âŒ ì‹¤íŒ¨ |
| Multi-head SHORT ë¶•ê´´ í•´ê²° | 8.97% â†’ 15.17% (ì¼ë¶€ ê°œì„ , ì—¬ì „íˆ ë¶€ì¡±) | â–³ ë¶€ë¶„ |
| ë‹¨ì¼ ëª¨ë¸ë¡œ ì–‘ìª½ ëª¨ë‘ baseline ì´ˆê³¼ | ê° attentionì´ í•œìª½ gateì—ë§Œ íŠ¹í™” | âŒ ì‹¤íŒ¨ |

**ê²°ë¡ :** concat poolingì€ **ê·¹ì ì¸ ê°œì„ ì„ ê°€ì ¸ì˜¤ì§€ ëª»í•¨**. ê·¼ë³¸ ë¬¸ì œ(ê° attention ë©”ì»¤ë‹ˆì¦˜ì´ íŠ¹ì • gateì—ë§Œ íŠ¹í™”)ëŠ” í•´ê²°ë˜ì§€ ì•ŠìŒ.

### ì„±ëŠ¥ ë¹„êµí‘œ

#### LONG GATE
| Metric | LSTM | Bahdanau | Multi-head | Best |
|--------|------|----------|------------|------|
| Label ratio | 17.57% | 17.57% | 17.57% | - |
| PR-AUC | 0.1987 | 0.1990 | **0.2130** | ğŸ† Multi-head |
| Coverage | 26.01% | 10.34% | 14.67% | LSTM |
| Precision | 21.10% | 19.35% | **23.63%** | ğŸ† Multi-head |
| Recall | 31.24% | 11.39% | 19.73% | LSTM |
| F1 | 25.19% | 14.34% | 21.51% | LSTM |
| **Precision@1%** | 23.45% | 20.69% | **48.28%** | ğŸ† Multi-head |
| Precision@5% | 21.54% | 20.03% | **27.85%** | ğŸ† Multi-head |
| Precision@10% | 21.93% | 19.40% | **26.05%** | ğŸ† Multi-head |
| Avg PnL | -0.1194% | -0.3416% | **-0.0708%** | ğŸ† Multi-head |
| Hit Rate | 48.71% | 45.46% | **50.63%** | ğŸ† Multi-head |
| **PnL@1%** | -0.0792% | +0.2891% | **+0.9902%** | ğŸ† Multi-head |
| PnL@5% | -0.1591% | -0.1976% | **+0.1362%** | ğŸ† Multi-head |
| PnL@10% | -0.1805% | -0.3305% | **+0.0336%** | ğŸ† Multi-head |

#### SHORT GATE
| Metric | LSTM | Bahdanau | Multi-head | Best |
|--------|------|----------|------------|------|
| Label ratio | 19.39% | 19.39% | 19.39% | - |
| PR-AUC | 0.2158 | **0.2546** | 0.2181 | ğŸ† Bahdanau |
| Coverage | 19.32% | 24.60% | 20.65% | Bahdanau |
| Precision | 23.84% | **26.13%** | 21.54% | ğŸ† Bahdanau |
| Recall | 23.75% | **33.16%** | 22.94% | ğŸ† Bahdanau |
| F1 | 23.80% | **29.23%** | 22.22% | ğŸ† Bahdanau |
| **Precision@1%** | 6.21% | **38.62%** | 15.17% | ğŸ† Bahdanau |
| Precision@5% | 16.74% | **34.02%** | 20.71% | ğŸ† Bahdanau |
| Precision@10% | 21.45% | **30.57%** | 21.38% | ğŸ† Bahdanau |
| Avg PnL | 0.1397% | **0.1575%** | 0.0921% | ğŸ† Bahdanau |
| Hit Rate | 54.74% | **54.76%** | 52.97% | ğŸ† Bahdanau |
| **PnL@1%** | -0.3799% | **+0.7869%** | -0.1700% | ğŸ† Bahdanau |
| PnL@5% | -0.1271% | **+0.4334%** | -0.0161% | ğŸ† Bahdanau |
| PnL@10% | +0.0283% | **+0.3061%** | +0.0595% | ğŸ† Bahdanau |

### í•µì‹¬ ë°œê²¬

#### 1. Attention ë©”ì»¤ë‹ˆì¦˜ë³„ Gate íŠ¹í™” í˜„ìƒ

```
Multi-head Attention â†’ LONGì— íŠ¹í™” (Precision@1% 48.28%, PnL@1% +0.99%)
Bahdanau Attention  â†’ SHORTì— íŠ¹í™” (Precision@1% 38.62%, PnL@1% +0.79%)
```

**ê°€ì„¤ì  í•´ì„:**
- LONGê³¼ SHORTì˜ ì‹œê³„ì—´ íŒ¨í„´ì´ **êµ¬ì¡°ì ìœ¼ë¡œ ë‹¤ë¥¼ ê°€ëŠ¥ì„±**
- Multi-head Self-Attention: ì—¬ëŸ¬ headê°€ **ë³‘ë ¬ì ìœ¼ë¡œ** ë‹¤ì–‘í•œ ê´€ê³„ í•™ìŠµ â†’ ìƒìŠ¹ ì „ "ë¶„ì‚°ëœ" íŒ¨í„´ì— ì í•©?
- Bahdanau Additive Attention: ë‹¨ì¼ query ê¸°ë°˜ **ìˆœì°¨ì ** ì¤‘ìš”ë„ ê³„ì‚° â†’ í•˜ë½ ì „ "ì§‘ì¤‘ëœ" íŒ¨í„´ì— ì í•©?

#### 2. 016 ìˆ˜ì • íš¨ê³¼ (concat pooling) - ì œí•œì 

**ì´ì „ ì‹¤í—˜ (avg pooling) vs ì´ë²ˆ ì‹¤í—˜ (concat pooling):**

| Gate | ì´ì „ Multi-head | ì´ë²ˆ Multi-head | ë³€í™” |
|------|----------------|-----------------|------|
| LONG Precision@1% | 44.14% | 48.28% | +4.1%p |
| SHORT Precision@1% | 8.97% | 15.17% | +6.2%p |

- concat poolingì´ SHORT ë¶•ê´´ë¥¼ **ì¼ë¶€ ì™„í™”**í–ˆìœ¼ë‚˜ **ê·¹ì  ê°œì„  ì•„ë‹˜**
- SHORT 15.17%ëŠ” ì´ì „ baseline(36.55%)ì— í•œì°¸ ëª» ë¯¸ì¹¨
- **last_hidden ë³´ì¡´ë§Œìœ¼ë¡œëŠ” ê·¼ë³¸ ë¬¸ì œ í•´ê²° ë¶ˆê°€**
- ê°€ì„¤ 1a/1b(ìš”ì•½ ë°©ì‹ ë³€ê²½)ê°€ í•µì‹¬ ì›ì¸ì´ ì•„ë‹ ìˆ˜ ìˆìŒ

#### 3. Baseline LSTMì˜ ì˜ì™¸ì˜ ì•½ì 

ì´ë²ˆ ì‹¤í—˜ì—ì„œ baseline LSTMì´ íŠ¹íˆ ì•½í•œ ë¶€ë¶„:
- **SHORT Precision@1%: 6.21%** (ê±°ì˜ ëœë¤ ìˆ˜ì¤€)
- **LONG PnL: ì „ êµ¬ê°„ ë§ˆì´ë„ˆìŠ¤**

**ê°€ì„¤:** ì´ë²ˆ í•™ìŠµì—ì„œ baselineì´ ìˆ˜ë ´ì´ ì˜ ì•ˆ ëê±°ë‚˜, ëœë¤ ì‹œë“œ ì˜í–¥ì¼ ìˆ˜ ìˆìŒ

### ê°€ì„¤ì  ë°©í–¥ì„±

#### ë°©í–¥ 1: Multi-head LONG + Bahdanau SHORT ì•™ìƒë¸” (Quick Win)

```python
# ê°€ì¥ ë‹¨ìˆœí•œ ì•™ìƒë¸”
long_pred = multihead_model.predict(X)['long_gate']
short_pred = bahdanau_model.predict(X)['short_gate']
```

**ì˜ˆìƒ ì„±ëŠ¥:**
- LONG Precision@1%: ~48% (Multi-head)
- SHORT Precision@1%: ~38% (Bahdanau)
- ì–‘ìª½ ëª¨ë‘ ë†’ì€ PnL@1% (+0.99%, +0.79%)

#### ë°©í–¥ 2: Gateë³„ Attention ë¶„ë¦¬ (ì•„í‚¤í…ì²˜ ìˆ˜ì •)

```python
# LONG gate: Multi-head Attention branch
long_attn = MultiHeadAttention(...)(lstm_out, lstm_out)
long_pooled = concat([last_hidden, GlobalAvgPool(long_attn)])

# SHORT gate: Bahdanau Attention branch
short_context, _ = BahdanauAttention(...)(last_hidden, lstm_out)
short_pooled = concat([last_hidden, short_context])

# ê° branchê°€ í•´ë‹¹ gateë§Œ ë‹´ë‹¹
long_gate = Dense(1, 'sigmoid')(long_branch)
short_gate = Dense(1, 'sigmoid')(short_branch)
```

#### ë°©í–¥ 3: ì¶”ê°€ ê²€ì¦ í•„ìš” ì‚¬í•­

1. **ì¬í˜„ì„± í™•ì¸**: ë™ì¼ ì„¤ì •ìœ¼ë¡œ 2-3íšŒ ì¶”ê°€ í•™ìŠµí•˜ì—¬ íŒ¨í„´ ì¼ê´€ì„± ê²€ì¦
2. **Pooling ablation**: Multi-headì—ì„œ `pooling_type='attention'`ë„ ì‹œë„ (SHORT ê°œì„  ê°€ëŠ¥ì„±)
3. **Bahdanau LONG ê°œì„ **: Bahdanauê°€ SHORTì—ë§Œ ì˜ ì‘ë™í•˜ëŠ” ì´ìœ  ë¶„ì„

### ìš”ì•½

| ë°œê²¬ | í™•ì‹ ë„ | ì‹œì‚¬ì  |
|------|--------|--------|
| **016 ìˆ˜ì •ì€ ê·¹ì  ê°œì„  ì—†ìŒ** | â­â­â­ ë†’ìŒ | ê°€ì„¤ 1a/1b(ìš”ì•½ ë°©ì‹ ë³€ê²½)ê°€ í•µì‹¬ ì›ì¸ì´ ì•„ë‹ ìˆ˜ ìˆìŒ |
| Multi-headëŠ” LONGì— íŠ¹í™” | â­â­â­ ë†’ìŒ | ì•™ìƒë¸”ì—ì„œ LONG ë‹´ë‹¹ ê°€ëŠ¥ |
| BahdanauëŠ” SHORTì— íŠ¹í™” | â­â­â­ ë†’ìŒ | ì•™ìƒë¸”ì—ì„œ SHORT ë‹´ë‹¹ ê°€ëŠ¥ |
| LONG/SHORT íŒ¨í„´ êµ¬ì¡° ì°¨ì´ | â­ ê°€ì„¤ | ë‹¤ë¥¸ ì›ì¸(gradient conflict, ë°ì´í„° íŠ¹ì„±) ê°€ëŠ¥ |

**í˜„ì‹¤ì  ëŒ€ì•ˆ:** ë‹¨ì¼ ëª¨ë¸ ê°œì„ ë³´ë‹¤ Multi-head LONG + Bahdanau SHORT ì•™ìƒë¸”ì´ í˜„ì‹¤ì 

