---
nav_order: 90
parent: 프로젝트
title: '[BACKLOG] 008 - Model Ensemble and Uncertainty Estimation'
description: "모델 앙상블과 불확실성 추정. Monte Carlo Dropout, Deep Ensemble. Temperature Scaling 보정. 확신도 기반 포지션 사이징."
---

# 008 - Model Ensemble and Uncertainty Estimation
{:.no_toc}

## 목차
{:.no_toc}

1. TOC
{:toc}

---

| 상태 | 생성일 | 수정일 |
|------|--------|--------|
| backlog | 2025-12-18 | - |

---

## Summary

**모델 앙상블**과 **불확실성 추정**을 도입하여 예측 신뢰도를 높이고, 확신도가 높을 때만 거래하는 전략 강화. Monte Carlo Dropout, Deep Ensemble, Calibration 기법 적용.

## Background

### Problem Statement
- 단일 모델의 과신(overconfidence) 문제
- softmax 확률이 실제 정확도와 불일치 (miscalibration)
- 불확실한 상황에서도 높은 확률 출력 → 잘못된 거래 신호

### Target State
- 여러 모델의 앙상블로 robust한 예측
- 불확실성 정량화 (epistemic + aleatoric)
- 확신도 높은 예측에서만 거래 → precision 향상

## Implementation Plan

### Phase 1: Monte Carlo Dropout

```python
class MCDropoutPredictor:
    """
    Monte Carlo Dropout: 추론 시에도 Dropout 활성화하여
    여러 번 예측 → 분포 추정
    """
    def __init__(self, model, n_samples: int = 30):
        self.model = model
        self.n_samples = n_samples

    def predict_with_uncertainty(self, X: np.ndarray) -> dict:
        """
        MC Dropout 예측
        """
        # Dropout을 활성화한 상태로 여러 번 예측
        predictions = []
        for _ in range(self.n_samples):
            # training=True로 Dropout 활성화
            pred = self.model(X, training=True)
            predictions.append(pred)

        predictions = np.array(predictions)  # (n_samples, batch, outputs)

        # 통계량 계산
        mean_pred = np.mean(predictions, axis=0)
        std_pred = np.std(predictions, axis=0)

        # Epistemic uncertainty (모델 불확실성)
        epistemic = std_pred

        # Predictive entropy (예측 엔트로피)
        entropy = -np.sum(mean_pred * np.log(mean_pred + 1e-10), axis=-1)

        return {
            'mean': mean_pred,
            'std': std_pred,
            'epistemic_uncertainty': epistemic,
            'entropy': entropy
        }
```

### Phase 2: Deep Ensemble

```python
class DeepEnsemble:
    """
    Deep Ensemble: 서로 다른 초기화로 학습된 여러 모델의 앙상블
    """
    def __init__(self, n_models: int = 5):
        self.n_models = n_models
        self.models = []

    def train(self, X_train, y_train, X_val, y_val, model_factory):
        """
        n_models개의 모델을 각각 다른 seed로 학습
        """
        for i in range(self.n_models):
            tf.random.set_seed(SEED + i)
            np.random.seed(SEED + i)

            model = model_factory()
            model.fit(X_train, y_train, validation_data=(X_val, y_val), ...)

            self.models.append(model)

    def predict(self, X: np.ndarray) -> dict:
        """
        앙상블 예측
        """
        predictions = [model.predict(X) for model in self.models]
        predictions = np.array(predictions)

        # Ensemble mean and std
        mean_pred = np.mean(predictions, axis=0)
        std_pred = np.std(predictions, axis=0)

        # Disagreement (모델 간 불일치)
        disagreement = np.mean([
            np.sum((p - mean_pred) ** 2) for p in predictions
        ])

        return {
            'mean': mean_pred,
            'std': std_pred,
            'disagreement': disagreement,
            'individual_preds': predictions
        }
```

### Phase 3: Temperature Scaling (Calibration)

```python
class TemperatureScaling:
    """
    Temperature Scaling: softmax 확률 보정
    높은 T → 확률 균등화, 낮은 T → 확률 첨예화
    """
    def __init__(self):
        self.temperature = 1.0

    def fit(self, logits: np.ndarray, labels: np.ndarray):
        """
        Validation set에서 최적 temperature 찾기
        """
        def nll_loss(T):
            scaled_probs = tf.nn.softmax(logits / T)
            return tf.reduce_mean(
                tf.keras.losses.categorical_crossentropy(labels, scaled_probs)
            ).numpy()

        # Grid search or optimization
        from scipy.optimize import minimize_scalar
        result = minimize_scalar(nll_loss, bounds=(0.1, 10.0), method='bounded')
        self.temperature = result.x

        return self.temperature

    def calibrate(self, logits: np.ndarray) -> np.ndarray:
        """
        보정된 확률 반환
        """
        return tf.nn.softmax(logits / self.temperature).numpy()
```

### Phase 4: Uncertainty-aware Trading

```python
def uncertainty_filter(predictions: dict, config: dict) -> np.ndarray:
    """
    불확실성 기반 거래 필터
    """
    gate_mean = predictions['gate']['mean']
    gate_std = predictions['gate']['std']
    entropy = predictions['entropy']

    # 1. Gate probability threshold
    gate_mask = gate_mean > config['gate_threshold']

    # 2. Low uncertainty filter (확신 있는 예측만)
    uncertainty_mask = gate_std < config['max_uncertainty']

    # 3. Low entropy filter (명확한 예측만)
    entropy_mask = entropy < config['max_entropy']

    # 4. Combined filter
    trade_mask = gate_mask & uncertainty_mask & entropy_mask

    return trade_mask

def compute_position_size(uncertainty: float, base_size: float,
                          min_size: float = 0.1, max_size: float = 2.0) -> float:
    """
    불확실성에 반비례하는 포지션 사이즈
    """
    # 불확실성이 낮을수록 큰 포지션
    confidence = 1.0 / (1.0 + uncertainty)
    size = base_size * confidence
    return np.clip(size, min_size, max_size)
```

### Phase 5: Calibration Metrics

```python
def compute_calibration_metrics(probs: np.ndarray, labels: np.ndarray,
                                n_bins: int = 10) -> dict:
    """
    Calibration 지표 계산
    """
    # Expected Calibration Error (ECE)
    bin_boundaries = np.linspace(0, 1, n_bins + 1)
    ece = 0.0
    mce = 0.0  # Maximum Calibration Error

    reliability_data = []

    for i in range(n_bins):
        in_bin = (probs >= bin_boundaries[i]) & (probs < bin_boundaries[i + 1])
        prop_in_bin = np.mean(in_bin)

        if prop_in_bin > 0:
            avg_confidence = np.mean(probs[in_bin])
            avg_accuracy = np.mean(labels[in_bin] == np.round(probs[in_bin]))

            ece += np.abs(avg_accuracy - avg_confidence) * prop_in_bin
            mce = max(mce, np.abs(avg_accuracy - avg_confidence))

            reliability_data.append({
                'bin': i,
                'confidence': avg_confidence,
                'accuracy': avg_accuracy,
                'count': np.sum(in_bin)
            })

    return {
        'ece': ece,
        'mce': mce,
        'reliability_diagram': reliability_data
    }
```

### Phase 6: Ensemble Training Pipeline

```python
def train_ensemble(handler: ModelHandler, X_train, y_train, X_val, y_val,
                   n_models: int = 5, save_dir: str = 'models/ensemble'):
    """
    앙상블 모델 학습 파이프라인
    """
    os.makedirs(save_dir, exist_ok=True)
    model_paths = []

    for i in range(n_models):
        logger.info(f"Training ensemble member {i+1}/{n_models}")

        # Different random seed
        tf.random.set_seed(SEED + i * 1000)
        np.random.seed(SEED + i * 1000)

        # Bootstrap sampling (optional)
        if config.get('bootstrap', True):
            indices = np.random.choice(len(X_train), len(X_train), replace=True)
            X_boot = X_train[indices]
            y_boot = y_train[indices]
        else:
            X_boot, y_boot = X_train, y_train

        # Train
        handler.weights_path = None
        handler.run_training_with_data(X_boot, y_boot, X_val, y_val)

        # Save
        model_path = f"{save_dir}/model_{i}.weights.h5"
        handler.model.save_weights(model_path)
        model_paths.append(model_path)

    # Save ensemble config
    with open(f"{save_dir}/ensemble_config.json", 'w') as f:
        json.dump({'n_models': n_models, 'model_paths': model_paths}, f)

    return model_paths
```

## Configuration

```python
ENSEMBLE_CONFIG = {
    'n_models': 5,              # 앙상블 모델 수
    'mc_samples': 30,           # MC Dropout 샘플 수
    'bootstrap': True,          # Bootstrap 샘플링 사용
    'temperature_scaling': True,

    # Uncertainty thresholds
    'max_uncertainty': 0.3,     # 최대 허용 불확실성
    'max_entropy': 1.0,         # 최대 허용 엔트로피

    # Position sizing
    'uncertainty_scaling': True,
    'min_position': 0.1,
    'max_position': 2.0,
}
```

## Success Criteria

- [ ] ECE (Expected Calibration Error) < 0.05
- [ ] Precision@10% 개선: +10% (불확실성 필터링 후)
- [ ] 고확신 거래의 win rate > 60%
- [ ] 앙상블 vs 단일 모델 Sharpe 개선: +0.3

## Files to Create/Modify

- `src/prediction/ensemble.py` (신규)
- `src/prediction/uncertainty.py` (신규)
- `src/prediction/calibration.py` (신규)
- `src/prediction/prediction_model.py` - 앙상블 연동
- `scripts/train_ensemble.py` (신규)

## Dependencies

- [003-1-selective-trading-gate-action](003-1-selective-trading-gate-action.md) - Gate+Action 구조
- [004-quantile-pnl-output-metrics](004-quantile-pnl-output-metrics.md) - Quantile 예측
- [005-backtesting-framework](005-backtesting-framework.md) - 성과 검증

## Estimated Scope

- MC Dropout: ~60 lines
- Deep Ensemble: ~100 lines
- Temperature Scaling: ~50 lines
- Uncertainty trading: ~80 lines
- Calibration metrics: ~50 lines
- Training pipeline: ~80 lines
- **Total: ~420 lines**

## References

- Gal & Ghahramani, "Dropout as a Bayesian Approximation"
- Lakshminarayanan et al., "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles"
- Guo et al., "On Calibration of Modern Neural Networks"

## Related Tickets

- [003-1-selective-trading-gate-action](003-1-selective-trading-gate-action.md) - Gate 확률 보정 대상
- [004-quantile-pnl-output-metrics](004-quantile-pnl-output-metrics.md) - Quantile 분포와 결합
- [005-backtesting-framework](005-backtesting-framework.md) - 불확실성 기반 포지션 사이징 테스트
- [006-advanced-loss-functions](006-advanced-loss-functions.md) - 불확실성 기반 loss weighting
- [007-regime-detection-adaptive](007-regime-detection-adaptive.md) - 레짐별 불확실성 다를 수 있음

