---
nav_order: 130
parent: 프로젝트
title: ' Temporal Fusion Transformer (TFT) 모델 실험'
description: "Temporal Fusion Transformer(TFT) 구현. Google 개발 시계열 SOTA 모델. Variable Selection Network, Interpretable Attention. Quantile 예측으로 불확실성 추정."
---

# Temporal Fusion Transformer (TFT) 모델 실험
{:.no_toc}

## 목차
{:.no_toc}

1. TOC
{:toc}

---

| 상태 | 생성일 | 수정일 |
|------|--------|--------|
| incomplete | 2025-12-18 | - |

---

## 개요
Google에서 개발한 Temporal Fusion Transformer를 적용하여 시계열 예측 성능을 개선하는 실험.

## 배경
- TFT는 다양한 유형의 입력(static, known future, observed)을 구분하여 처리
- Variable Selection Network로 중요 feature 자동 선택
- Interpretable Multi-Head Attention으로 시간 의존성 학습
- Quantile 예측을 통한 불확실성 추정 가능

## 모델 아키텍처

### 핵심 구성요소

1. **Variable Selection Networks (VSN)**
   - 각 시점에서 중요한 변수를 자동 선택
   - Softmax 가중치로 변수별 중요도 학습
   - Feature importance 해석 가능

2. **Gated Residual Networks (GRN)**
   - 비선형 처리를 위한 게이트 메커니즘
   - Residual connection으로 기울기 흐름 개선
   - 전체 아키텍처에서 빌딩 블록으로 사용

3. **Static Covariate Encoders**
   - 시간에 따라 변하지 않는 특성 인코딩
   - 예: 자산 종류, 시장 특성 등

4. **Temporal Self-Attention**
   - Interpretable Multi-Head Attention
   - 각 시점의 기여도를 해석 가능
   - Decoder masking으로 미래 정보 누출 방지

5. **Point-wise Feed-Forward**
   - 최종 예측을 위한 비선형 변환

### 구조
```
Inputs:
├── Static Covariates (시간 불변)
├── Known Future Inputs (미래 알 수 있는: 시간 인코딩)
└── Observed Past Inputs (과거 관측: 가격, 지표)
    ↓
Variable Selection Networks
    ↓
Static Covariate Encoders
    ↓
LSTM Encoder (과거) + LSTM Decoder (미래)
    ↓
Static Enrichment (GRN)
    ↓
Temporal Self-Attention
    ↓
Position-wise Feed-Forward
    ↓
Quantile Outputs / Point Predictions
```

## 구현 계획

### Phase 1: GRN 및 VSN 구현
```python
class GatedResidualNetwork(tf.keras.layers.Layer):
    def __init__(self, hidden_size, output_size=None, dropout=0.1):
        super().__init__()
        self.hidden_size = hidden_size
        self.output_size = output_size or hidden_size

        self.dense1 = Dense(hidden_size, activation='elu')
        self.dense2 = Dense(hidden_size)
        self.gate = Dense(self.output_size, activation='sigmoid')
        self.dense_out = Dense(self.output_size)
        self.dropout = Dropout(dropout)
        self.layer_norm = LayerNormalization()

    def call(self, x, context=None, training=False):
        # Optional context (static covariates)
        if context is not None:
            x = tf.concat([x, context], axis=-1)

        hidden = self.dense1(x)
        hidden = self.dropout(hidden, training=training)
        hidden = self.dense2(hidden)

        gating = self.gate(hidden)
        output = self.dense_out(hidden)

        # Gated skip connection
        output = gating * output + (1 - gating) * x[..., :self.output_size]
        return self.layer_norm(output)


class VariableSelectionNetwork(tf.keras.layers.Layer):
    def __init__(self, hidden_size, num_inputs, dropout=0.1):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_inputs = num_inputs

        # Individual GRNs for each input
        self.grns = [GatedResidualNetwork(hidden_size, dropout=dropout)
                     for _ in range(num_inputs)]

        # GRN for variable selection weights
        self.grn_flat = GatedResidualNetwork(hidden_size, dropout=dropout)
        self.softmax = Dense(num_inputs, activation='softmax')

    def call(self, inputs, context=None, training=False):
        # inputs: list of (batch, time, features) tensors
        processed = []
        for i, (inp, grn) in enumerate(zip(inputs, self.grns)):
            processed.append(grn(inp, context, training=training))

        # Stack and compute weights
        stacked = tf.stack(processed, axis=-1)  # (batch, time, hidden, num_inputs)

        # Flatten for selection weights
        flat = tf.reshape(tf.concat(inputs, axis=-1), [-1, self.num_inputs * inputs[0].shape[-1]])
        weights = self.softmax(self.grn_flat(flat, context, training=training))

        # Weighted combination
        weights = tf.reshape(weights, [-1, 1, 1, self.num_inputs])
        output = tf.reduce_sum(stacked * weights, axis=-1)

        return output, weights
```

### Phase 2: Interpretable Multi-Head Attention
```python
class InterpretableMultiHeadAttention(tf.keras.layers.Layer):
    def __init__(self, n_heads, d_model, dropout=0.1):
        super().__init__()
        self.n_heads = n_heads
        self.d_model = d_model
        self.d_k = d_model // n_heads

        self.W_q = Dense(d_model)
        self.W_k = Dense(d_model)
        self.W_v = Dense(d_model)
        self.W_o = Dense(d_model)

        self.dropout = Dropout(dropout)

    def call(self, query, key, value, mask=None, training=False):
        batch_size = tf.shape(query)[0]

        # Linear projections
        Q = self.W_q(query)
        K = self.W_k(key)
        V = self.W_v(value)

        # Reshape for multi-head
        Q = tf.reshape(Q, [batch_size, -1, self.n_heads, self.d_k])
        K = tf.reshape(K, [batch_size, -1, self.n_heads, self.d_k])
        V = tf.reshape(V, [batch_size, -1, self.n_heads, self.d_k])

        Q = tf.transpose(Q, [0, 2, 1, 3])
        K = tf.transpose(K, [0, 2, 1, 3])
        V = tf.transpose(V, [0, 2, 1, 3])

        # Attention scores
        scores = tf.matmul(Q, K, transpose_b=True) / tf.sqrt(float(self.d_k))

        if mask is not None:
            scores += mask * -1e9

        attention_weights = tf.nn.softmax(scores, axis=-1)
        attention_weights = self.dropout(attention_weights, training=training)

        # Weighted values
        context = tf.matmul(attention_weights, V)
        context = tf.transpose(context, [0, 2, 1, 3])
        context = tf.reshape(context, [batch_size, -1, self.d_model])

        output = self.W_o(context)

        # Average attention weights across heads for interpretability
        avg_attention = tf.reduce_mean(attention_weights, axis=1)

        return output, avg_attention
```

### Phase 3: 전체 TFT 모델 통합
- Static covariate encoder 추가
- LSTM encoder-decoder 구현
- Quantile output 레이어 추가
- Multi-task head 연결

### Phase 4: 하이퍼파라미터 튜닝
- hidden_size: 64, 128, 256
- n_heads: 4, 8
- LSTM layers: 1, 2
- dropout: 0.1, 0.2, 0.3

## 입력 변수 분류

현재 모델의 feature들을 TFT 입력 유형에 맞게 분류:

| 유형 | 변수 | 설명 |
|------|------|------|
| Static | - | 현재 없음 (추후 자산별 특성 추가 가능) |
| Known Future | time_sin, time_cos, day_sin, day_cos | 시간 인코딩 (미래 시점도 계산 가능) |
| Observed Past | price returns, MACD, RSI, ATR, volume, OI, funding rate | 과거에만 관측 가능 |

## 변경 파일
- `src/prediction/prediction_model.py`
  - GRN, VSN 레이어 클래스 추가
  - InterpretableMultiHeadAttention 구현
  - TemporalFusionTransformer 메인 클래스
  - Feature를 static/known/observed로 분리하는 로직

## 평가 기준
- Classification Accuracy (Close/High/Low) > 40%
- Close Direction Accuracy > 55%
- Selective Trading PnL > 0%
- Variable importance 분석
- Attention weights 시각화

## 참고 자료

### 논문
1. **"Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting"** (Lim et al., 2021)
   - TFT 원본 논문 (Google Research)
   - https://arxiv.org/abs/1912.09363
   - International Journal of Forecasting 게재

2. **"Deep Learning for Time Series Forecasting: Tutorial and Literature Survey"** (Lim & Zohren, 2021)
   - 시계열 딥러닝 종합 서베이
   - TFT 포함 다양한 아키텍처 비교
   - https://arxiv.org/abs/2004.13408

### 공식 구현
- **Google Research TFT**: https://github.com/google-research/google-research/tree/master/tft
  - 공식 TensorFlow 구현

- **PyTorch Forecasting**: https://pytorch-forecasting.readthedocs.io/en/stable/models.html
  - TFT의 PyTorch 구현 (참고용)

### TensorFlow 관련
- [Keras Time Series Tutorial](https://www.tensorflow.org/tutorials/structured_data/time_series)
- [tf.keras.layers.LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM)

### 블로그 및 튜토리얼
- [TFT Paper Explained (Towards Data Science)](https://towardsdatascience.com/temporal-fusion-transformers-for-interpretable-multi-horizon-time-series-forecasting-6fec4b0d4e01)
- [Understanding TFT Architecture](https://unit8.com/resources/temporal-fusion-transformer-a-primer-on-deep-forecasting-in-python/)
- [TFT Implementation Guide (Neptune.ai)](https://neptune.ai/blog/temporal-fusion-transformer)

## TFT의 주요 장점

| 특징 | 설명 |
|------|------|
| Multi-horizon | 여러 시점 동시 예측 |
| Interpretability | Variable selection, attention weights로 해석 가능 |
| Multiple input types | Static, known future, observed past 구분 |
| Quantile forecasting | 불확실성 추정 가능 |
| State-of-the-art | M4, 전력 수요 등 벤치마크 1위 |

## 체크리스트
- [ ] GatedResidualNetwork 구현
- [ ] VariableSelectionNetwork 구현
- [ ] InterpretableMultiHeadAttention 구현
- [ ] Static Covariate Encoder 구현
- [ ] LSTM Encoder-Decoder 구현
- [ ] Feature를 static/known/observed로 분류
- [ ] 전체 TFT 모델 통합
- [ ] Multi-task output head 연결
- [ ] 학습 및 평가 실행
- [ ] Variable importance 시각화
- [ ] Attention 패턴 분석
- [ ] LSTM/PatchTST 대비 성능 비교

