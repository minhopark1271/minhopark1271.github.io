---
nav_order: 140
parent: 프로젝트
title: ' PatchTST 모델 실험'
---

# PatchTST 모델 실험
{:.no_toc}

## 목차
{:.no_toc}

1. TOC
{:toc}

---

| 상태 | 생성일 | 수정일 |
|------|--------|--------|
| incomplete | 2025-12-18 | - |

---

## 개요
시계열을 패치(patch) 단위로 분할하여 Transformer로 처리하는 PatchTST 아키텍처 실험.

## 배경
- PatchTST는 시계열을 패치로 나누어 처리하여 계산 효율성과 성능을 모두 개선
- Channel-Independence: 각 변수를 독립적으로 처리하여 overfitting 방지
- 2023년 발표 이후 다양한 시계열 벤치마크에서 SOTA 달성

## 모델 아키텍처

### 핵심 개념

1. **Patching**
   - 시계열을 겹치는/겹치지 않는 패치로 분할
   - 예: 96 timesteps → 8 patches (patch_len=16, stride=8)
   - 패치 단위로 Transformer에 입력

2. **Channel-Independence**
   - 각 feature(채널)를 독립적인 시퀀스로 처리
   - 공유된 Transformer backbone 사용
   - Multivariate 문제를 Univariate처럼 처리

3. **Instance Normalization**
   - 배치 정규화 대신 인스턴스 정규화 사용
   - 분포 이동(distribution shift)에 강건

### 구조
```
Input (96 timesteps × N features)
    ↓
[Channel Independence] N개의 univariate 시퀀스로 분리
    ↓
Patching: (96,) → (num_patches, patch_len)
    ↓
Patch Embedding (Linear projection)
    ↓
Positional Encoding
    ↓
Transformer Encoder (L layers)
    ↓
Flatten → Linear → Prediction
    ↓
[Combine Channels] Multi-task Outputs
```

## 구현 계획

### Phase 1: 기본 PatchTST 구현
```python
class PatchTST(tf.keras.Model):
    def __init__(self,
                 seq_len=96,
                 patch_len=16,
                 stride=8,
                 d_model=128,
                 n_heads=4,
                 n_layers=3,
                 d_ff=256,
                 dropout=0.2):
        super().__init__()
        self.patch_len = patch_len
        self.stride = stride
        self.num_patches = (seq_len - patch_len) // stride + 1

        # Patch embedding
        self.patch_embedding = Dense(d_model)

        # Positional encoding
        self.pos_encoding = self.positional_encoding(self.num_patches, d_model)

        # Transformer encoder layers
        self.encoder_layers = [
            TransformerEncoderLayer(d_model, n_heads, d_ff, dropout)
            for _ in range(n_layers)
        ]

        # Output projection
        self.flatten = Flatten()
        self.output_projection = Dense(pred_len)
```

### Phase 2: Channel-Independent 처리
```python
def call(self, x):
    # x: (batch, seq_len, n_features)
    batch_size, seq_len, n_features = x.shape

    # Channel independence: process each feature separately
    outputs = []
    for i in range(n_features):
        channel = x[:, :, i:i+1]  # (batch, seq_len, 1)

        # Create patches
        patches = self.create_patches(channel)  # (batch, num_patches, patch_len)

        # Embed patches
        embedded = self.patch_embedding(patches)  # (batch, num_patches, d_model)
        embedded = embedded + self.pos_encoding

        # Transformer encoding
        for layer in self.encoder_layers:
            embedded = layer(embedded)

        outputs.append(embedded)

    # Combine channel outputs
    combined = tf.concat(outputs, axis=-1)
    return self.output_head(combined)
```

### Phase 3: 하이퍼파라미터 튜닝
- patch_len: 8, 16, 24
- stride: patch_len // 2 (50% overlap)
- d_model: 64, 128, 256
- n_heads: 2, 4, 8
- n_layers: 2, 3, 4

## 변경 파일
- `src/prediction/prediction_model.py`
  - PatchTST 클래스 추가
  - TransformerEncoderLayer 구현
  - 패치 생성 유틸리티 함수
  - 모델 선택 파라미터 추가

## 평가 기준
- Classification Accuracy (Close/High/Low) > 40%
- Close Direction Accuracy > 55%
- Selective Trading PnL > 0%
- 학습 시간 비교 (LSTM 대비)
- 메모리 사용량 비교

## 참고 자료

### 논문
1. **"A Time Series is Worth 64 Words: Long-term Forecasting with Transformers"** (Nie et al., 2023)
   - PatchTST 원본 논문
   - https://arxiv.org/abs/2211.14730
   - ICLR 2023 발표

2. **"Are Transformers Effective for Time Series Forecasting?"** (Zeng et al., 2023)
   - 시계열 Transformer 비교 연구
   - Linear 모델과의 비교
   - https://arxiv.org/abs/2205.13504

### 공식 구현
- **PatchTST Official GitHub**: https://github.com/yuqinie98/PatchTST
  - PyTorch 공식 구현
  - 다양한 벤치마크 실험 코드

### TensorFlow 관련
- [Keras Transformer Encoder](https://keras.io/api/layers/core_layers/embedding/)
- [tf.keras.layers.MultiHeadAttention](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention)

### 블로그 및 튜토리얼
- [PatchTST Paper Explained (Medium)](https://medium.com/@zeng.yuan98/patchtst-a-time-series-is-worth-64-words-long-term-forecasting-with-transformers-e3fc1eb3b7c0)
- [Hugging Face Time Series Transformer](https://huggingface.co/blog/time-series-transformers)

## 주요 특징 요약

| 특징 | 설명 |
|------|------|
| Patching | 시계열을 subseries로 분할하여 Transformer 입력으로 사용 |
| Channel Independence | 각 변수를 독립 처리하여 overfitting 방지 |
| Instance Norm | RevIN 적용으로 distribution shift에 강건 |
| Efficiency | 패치 기반으로 attention 계산 복잡도 감소 |

## 체크리스트
- [ ] PatchTST 기본 클래스 구현
- [ ] Patching 레이어 구현
- [ ] Channel-Independent 처리 구현
- [ ] Transformer Encoder 구현
- [ ] Instance Normalization (RevIN) 추가
- [ ] Multi-task output head 연결
- [ ] 학습 및 평가 실행
- [ ] 패치 크기별 성능 비교
- [ ] LSTM 대비 성능/효율성 비교

