---
nav_order: 120
parent: 프로젝트
title: '[BACKLOG] 006 - Advanced Loss Functions (Focal, PnL-weighted)'
description: "고급 손실 함수 구현. Focal Loss, PnL-weighted Loss, Asymmetric Loss. 클래스 불균형 해결 및 수익 극대화 방향 학습."
---

# 006 - Advanced Loss Functions (Focal, PnL-weighted)
{:.no_toc}

## 목차
{:.no_toc}

1. TOC
{:toc}

---

| 상태 | 생성일 | 수정일 |
|------|--------|--------|
| backlog | 2025-12-18 | - |

---

## Summary

Gate/Action 헤드의 학습 품질을 높이기 위해 **Focal Loss**, **PnL-weighted Loss**, **Asymmetric Loss** 등 고급 손실함수를 도입한다. 클래스 불균형 문제 해결 및 수익 극대화 방향으로 학습 유도.

## Background

### Problem Statement
- Gate=1 샘플이 전체의 10-20%로 불균형 → 모델이 Gate=0만 예측하는 붕괴 가능
- 동일한 오분류라도 **수익 기회 놓침 vs 손실 발생**의 비용이 다름
- 표준 CE/BCE는 이런 비대칭성을 반영하지 못함

### Target State
- Focal Loss로 어려운 샘플에 집중
- PnL-weighted Loss로 큰 수익 기회에 더 집중
- Asymmetric Loss로 손실 회피 강조

## Implementation Plan

### Phase 1: Focal Loss for Gate

```python
def focal_loss(gamma=2.0, alpha=0.25):
    """
    Focal Loss: 쉬운 샘플(확률 높은)의 loss를 줄여 어려운 샘플에 집중

    gamma: focusing parameter (높을수록 쉬운 샘플 무시)
    alpha: class weight for positive class
    """
    def loss(y_true, y_pred):
        y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)

        # Binary focal loss
        pt = tf.where(tf.equal(y_true, 1), y_pred, 1 - y_pred)
        alpha_t = tf.where(tf.equal(y_true, 1), alpha, 1 - alpha)

        focal_weight = alpha_t * tf.pow(1 - pt, gamma)
        ce = -tf.math.log(pt)

        return focal_weight * ce

    return loss
```

### Phase 2: PnL-weighted Loss for Action

```python
def pnl_weighted_ce(y_true, y_pred, pnl_values, gate_mask):
    """
    PnL 크기에 비례하여 loss weight 부여
    큰 수익 기회를 놓치면 더 큰 페널티

    pnl_values: 각 샘플의 실제 최적 PnL
    gate_mask: gate=1인 샘플만 강조
    """
    ce = tf.keras.losses.categorical_crossentropy(y_true, y_pred)

    # PnL 기반 weight (양수 PnL일수록 중요)
    pnl_weight = tf.maximum(pnl_values, 0.0) + 1.0  # 최소 1.0
    pnl_weight = pnl_weight / tf.reduce_mean(pnl_weight)  # 정규화

    # Gate mask 적용 (gate=1인 샘플만 강조)
    weighted_ce = ce * pnl_weight * gate_mask

    return tf.reduce_mean(weighted_ce)
```

### Phase 3: Asymmetric Loss

```python
def asymmetric_loss(y_true, y_pred, gamma_pos=0, gamma_neg=4):
    """
    Asymmetric Loss: False Negative와 False Positive에 다른 페널티

    거래 관점:
    - FN (실제 좋은 기회인데 놓침): gamma_pos 낮게 → 덜 페널티
    - FP (거래했는데 손실): gamma_neg 높게 → 더 큰 페널티
    """
    y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)

    # Positive samples (gate=1, 거래해야 할 시점)
    pos_loss = y_true * tf.pow(1 - y_pred, gamma_pos) * (-tf.math.log(y_pred))

    # Negative samples (gate=0, 거래하면 안 되는 시점)
    neg_loss = (1 - y_true) * tf.pow(y_pred, gamma_neg) * (-tf.math.log(1 - y_pred))

    return pos_loss + neg_loss
```

### Phase 4: Combined Multi-objective Loss

```python
class TradingLoss(tf.keras.losses.Loss):
    """
    통합 손실함수: PnL 최대화 + 리스크 제어 + 분류 품질
    """
    def __init__(self, alpha_pnl=1.0, alpha_risk=0.5, alpha_cls=1.0):
        super().__init__()
        self.alpha_pnl = alpha_pnl
        self.alpha_risk = alpha_risk
        self.alpha_cls = alpha_cls

    def call(self, y_true, y_pred, sample_weight=None):
        gate_true, action_true, pnl_true = y_true
        gate_pred, action_pred, pnl_pred = y_pred

        # 1. Classification loss (Focal)
        L_gate = focal_loss(gamma=2.0)(gate_true, gate_pred)
        L_action = pnl_weighted_ce(action_true, action_pred, pnl_true, gate_true)

        # 2. PnL maximization (예측 PnL과 실제 PnL 차이)
        L_pnl = tf.reduce_mean(tf.abs(pnl_true - pnl_pred))

        # 3. Risk penalty (큰 손실 예측 시 추가 페널티)
        L_risk = tf.reduce_mean(tf.maximum(-pnl_pred, 0.0))

        total = (self.alpha_cls * (L_gate + L_action) +
                 self.alpha_pnl * L_pnl +
                 self.alpha_risk * L_risk)

        return total
```

### Phase 5: Dynamic Loss Weight Scheduling

```python
class LossWeightScheduler(tf.keras.callbacks.Callback):
    """
    학습 진행에 따라 loss weight 동적 조정
    초기: classification 위주 → 후기: PnL 위주
    """
    def __init__(self, model, warmup_epochs=10):
        self.model = model
        self.warmup_epochs = warmup_epochs

    def on_epoch_begin(self, epoch, logs=None):
        if epoch < self.warmup_epochs:
            # Warmup: classification에 집중
            pnl_weight = 0.1 * (epoch / self.warmup_epochs)
        else:
            # Main training: PnL 비중 증가
            pnl_weight = 0.1 + 0.9 * min(1.0, (epoch - self.warmup_epochs) / 20)

        self.model.loss_weights['pnl'] = pnl_weight
```

## Configuration

```python
# Focal Loss
FOCAL_GAMMA = 2.0
FOCAL_ALPHA = 0.25  # gate=1 class weight

# Asymmetric Loss
ASYM_GAMMA_POS = 0  # 좋은 기회 놓침에 대한 페널티
ASYM_GAMMA_NEG = 4  # 잘못된 거래에 대한 페널티

# PnL weighting
PNL_WEIGHT_SCALE = 2.0  # PnL 기반 weight 스케일

# Loss combination
ALPHA_CLS = 1.0
ALPHA_PNL = 0.5
ALPHA_RISK = 0.3
```

## Success Criteria

- [ ] Gate precision 개선: +5% over baseline ([003-1-selective-trading-gate-action](003-1-selective-trading-gate-action.md))
- [ ] 클래스 불균형에도 Gate=1 예측 비율 유지 (10-30%)
- [ ] Action accuracy (gate=1) 개선: +3%
- [ ] Average PnL per trade 개선: +10%

## Files to Modify

- `src/prediction/prediction_model.py`
  - 새로운 loss function 추가
  - compile 시 custom loss 적용
  - Callback 추가

## Dependencies

- [003-1-selective-trading-gate-action](003-1-selective-trading-gate-action.md) - Gate+Action 구조
- [004-quantile-pnl-output-metrics](004-quantile-pnl-output-metrics.md) - PnL 평가 지표

## Estimated Scope

- Loss functions: ~100 lines
- Scheduler callback: ~30 lines
- Integration: ~30 lines
- **Total: ~160 lines**

## References

- Lin et al., "Focal Loss for Dense Object Detection" (RetinaNet)
- Ridnik et al., "Asymmetric Loss For Multi-Label Classification"

## Related Tickets

- [003-1-selective-trading-gate-action](003-1-selective-trading-gate-action.md) - 기본 Gate+Action 구조
- [004-quantile-pnl-output-metrics](004-quantile-pnl-output-metrics.md) - PnL 평가 지표
- [008-model-ensemble-uncertainty](008-model-ensemble-uncertainty.md) - 불확실성과 결합 가능

