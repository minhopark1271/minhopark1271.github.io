---
nav_order: 20
parent: 프로젝트
title: '[DONE] LSTM + Attention Hybrid 모델 실험'
description: "LSTM+Attention 하이브리드 모델 실험. Bahdanau Attention, Multi-Head Attention 구현. 희귀 패턴 탐지 성능 개선. PR-AUC, Precision@K 평가."
---

# LSTM + Attention Hybrid 모델 실험
{:.no_toc}

## 목차
{:.no_toc}

1. TOC
{:toc}

---

| 상태 | 생성일 | 수정일 |
|------|--------|--------|
| completed | 2025-12-18 | 2025-12-24 |

---

## 개요
현재 LSTM 기반 Long/Short Gate 모델에 Attention 메커니즘을 추가하여 예측 가능한 희귀 패턴(~1%) 탐지 성능을 개선하는 실험.

## 배경

### 현재 모델 구조 (003-3 완료 후)
- **2-head 구조**: long_gate, short_gate만 출력
- **라벨 정의**:
  - Long Gate: `(upside > downside * 2) & (upside_pct > 0.01)`
  - Short Gate: `(downside > upside * 2) & (downside_pct > 0.01)`
- **손실 함수**: BCE / Bootstrapped BCE / Self-paced BCE 선택 가능
- **평가 메트릭**: PR-AUC, Precision@K (1%, 5%, 10%), coverage 중심

### 문제 구조 (핵심)
- 전체 양성(gate=1)은 약 **10%**
- 입력 패턴으로 예측 가능한 양성은 약 **1%**
- 나머지 **9%는 노이즈** (입력과 독립적으로 발생)
- **목표**: 예측 가능한 1%만 높은 precision으로 탐지

### 가설
- Attention을 통해 **중요한 시간 단계에 집중**하면 노이즈에 덜 민감해지고 패턴 탐지 성능 향상 가능
- 특히 급등/급락 직전의 특징적인 시계열 패턴을 더 잘 포착할 수 있음

## 모델 아키텍처

### 현재 구조 (Baseline)
```
Input (96 timesteps × features)
    ↓
LSTM Layer (64 units, return_sequences=False)
    ↓
Dropout (0.3)
    ↓
Shared Dense Layers
    ↓
{long_gate, short_gate} (sigmoid)
```

### 제안 구조 (Attention 추가)
```
Input (96 timesteps × features)
    ↓
LSTM Layer (return_sequences=True)  ← 변경: 모든 hidden states 반환
    ↓
Self-Attention Layer
    ↓
Context Vector (weighted sum of LSTM outputs)
    ↓
Dropout (0.3)
    ↓
Shared Dense Layers
    ↓
{long_gate, short_gate} (sigmoid)
```

### 핵심 구성요소

1. **Bahdanau Attention (Additive Attention)**
   - Query: 마지막 hidden state
   - Keys/Values: 모든 LSTM hidden states
   - 각 시간 단계의 중요도를 학습

2. **Multi-Head Self-Attention (선택적)**
   - 여러 관점에서 시퀀스 내 관계 학습
   - Transformer의 핵심 메커니즘 적용

## 구현 계획

### Phase 1: Bahdanau Attention 추가
```python
class BahdanauAttention(tf.keras.layers.Layer):
    def __init__(self, units):
        super().__init__()
        self.W1 = Dense(units)
        self.W2 = Dense(units)
        self.V = Dense(1)

    def call(self, query, values):
        # query: (batch, hidden_size)
        # values: (batch, seq_len, hidden_size)
        query_with_time = tf.expand_dims(query, 1)
        score = self.V(tf.nn.tanh(self.W1(query_with_time) + self.W2(values)))
        attention_weights = tf.nn.softmax(score, axis=1)
        context = attention_weights * values
        context = tf.reduce_sum(context, axis=1)
        return context, attention_weights
```

### Phase 2: Keras MultiHeadAttention 활용
```python
# TensorFlow 내장 MultiHeadAttention 사용
attention_output = tf.keras.layers.MultiHeadAttention(
    num_heads=4,
    key_dim=32
)(lstm_output, lstm_output)
```

### Phase 3: 하이퍼파라미터 튜닝
- Attention units: 32, 64, 128
- Number of heads: 2, 4, 8
- LSTM units: 64, 128

## Multi-Head Attention 헤드 수 가이드라인

### 경험적 가이드라인 (주요 모델 참고)

| 출처 | 설정 | 비율 |
|------|------|------|
| Transformer 원논문 | d=512, 8 heads | 64-dim/head |
| BERT | d=768, 12 heads | 64-dim/head |
| GPT-2 | d=768, 12 heads | 64-dim/head |
| **일반적 권장** | - | **≥16-dim/head** |

### 현재 설정 분석

```python
# 현재 설정
LSTM_UNITS = 64      # LSTM output dimension
NUM_HEADS = 4        # Attention heads
KEY_DIM = 32         # Key/Query dimension per head
```

**권장 헤드 수:**
- 64-dim LSTM → 2~4개 헤드
- 128-dim LSTM → 4~8개 헤드

### 헤드 수 증가 시 고려사항

| Heads | key_dim (총 128 고정 시) | 판단 |
|-------|-------------------------|------|
| 4 | 32-dim/head | ✓ 적절 |
| 8 | 16-dim/head | ⚠️ 경계선 |
| 16 | 8-dim/head | ❌ 너무 작음 |
| 32 | 4-dim/head | ❌ 무의미 |

**문제점:**
- key_dim이 작아지면 각 헤드의 패턴 표현력 감소
- LSTM 차원과 불균형 시 오버피팅 위험
- 시계열 96 timesteps에 과도한 헤드 수는 불필요

### 의미 있는 확장 방법

헤드를 늘리려면 **LSTM 차원도 함께 확장** 필요:

```python
# Option A: 현재 (적절한 균형)
LSTM_UNITS = 64, NUM_HEADS = 4, KEY_DIM = 32
# → 71,042 params

# Option B: 헤드 증가 + LSTM 확장
LSTM_UNITS = 128, NUM_HEADS = 8, KEY_DIM = 32
# → ~150,000 params

# Option C: 더 큰 모델
LSTM_UNITS = 256, NUM_HEADS = 16, KEY_DIM = 32
# → ~400,000 params
```

### 튜닝 우선순위

1. 먼저 Bahdanau vs MultiHead 성능 비교
2. 개선 효과 있으면 LSTM 차원 확장 (64→128)
3. 그 후 헤드 수 튜닝 (4→8)

> **결론**: 현재 데이터/모델 규모에서는 **4 헤드가 적절**. 헤드만 늘리는 것보다 LSTM 차원 확장이 선행되어야 함.

## 변경 파일
- `src/prediction/prediction_model.py`
  - `create_model_lstm()` 함수 수정 (LSTM return_sequences=True)
  - Attention 레이어 클래스 추가
  - 하이퍼파라미터 상수 추가 (ATTENTION_UNITS, NUM_HEADS 등)

## 평가 기준 (003-3 메트릭 기준)

### 주요 지표
| 지표 | Baseline 목표 | Attention 목표 |
|------|--------------|----------------|
| Long Gate PR-AUC | 기록 | > Baseline |
| Short Gate PR-AUC | 기록 | > Baseline |
| Precision@1% | 기록 | > Baseline + 5%p |
| Precision@5% | 기록 | > Baseline + 3%p |
| PnL@1% | 기록 | > Baseline |

### 성공 기준
- [ ] Long Gate 또는 Short Gate 중 하나 이상에서 **PR-AUC 개선**
- [ ] **Precision@1%** 또는 **Precision@5%**에서 Baseline 대비 개선
- [ ] Attention weights 시각화로 **해석 가능성** 확보
- [ ] 학습 시간 2배 이내 유지

## 참고 자료

### 논문
1. **"Neural Machine Translation by Jointly Learning to Align and Translate"** (Bahdanau et al., 2014)
   - Additive Attention (Bahdanau Attention) 원본 논문
   - https://arxiv.org/abs/1409.0473

2. **"Attention Is All You Need"** (Vaswani et al., 2017)
   - Multi-Head Attention, Transformer 아키텍처
   - https://arxiv.org/abs/1706.03762

3. **"A Dual-Stage Attention-Based Recurrent Neural Network for Time Series Prediction"** (Qin et al., 2017)
   - 시계열 예측을 위한 DA-RNN
   - Input Attention + Temporal Attention
   - https://arxiv.org/abs/1704.02971

### TensorFlow 공식 문서
- [MultiHeadAttention Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention)
- [Attention Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention)

### 구현 가이드
- [TensorFlow Attention Tutorial](https://www.tensorflow.org/text/tutorials/nmt_with_attention)
- [Keras Attention Mechanism](https://keras.io/examples/nlp/text_classification_with_transformer/)

## 실험 순서

1. **Baseline 기록** (003-3 모델)
   - BCE / Bootstrapped / Self-paced 중 최적 손실 함수 선택
   - PR-AUC, Precision@K 기록

2. **Bahdanau Attention 실험**
   - Attention units: 32, 64
   - 동일 손실 함수로 비교

3. **Multi-Head Attention 실험**
   - num_heads: 2, 4
   - key_dim: 16, 32

4. **최종 비교 및 분석**
   - Baseline vs Attention 성능 비교
   - Attention weights 시각화 및 해석

## 체크리스트

### Phase 1: Bahdanau Attention
- [x] BahdanauAttention 레이어 클래스 구현
- [x] `create_model_lstm()`에 attention 옵션 추가
- [x] LSTM return_sequences=True로 변경 (attention 사용 시)
- [ ] Baseline 모델로 학습 및 PR-AUC/Precision@K 기록
- [ ] Attention 모델로 학습 및 비교

### Phase 2: Multi-Head Attention
- [x] Keras MultiHeadAttention 레이어 통합
- [ ] Self-Attention vs Bahdanau 성능 비교
- [ ] 최적 num_heads, key_dim 탐색

### Phase 3: 분석
- [ ] Attention weights 시각화 함수 구현
- [ ] 거래 신호 발생 시점의 attention 패턴 분석
- [ ] 결과 문서화

## 선행 작업
- [003-3-long-short-gate-separation](003-3-long-short-gate-separation.md) - Long/Short Gate 2-head 구조 (완료)

## 관련 티켓
- [003-1-selective-trading-gate-action](003-1-selective-trading-gate-action.md) - 제거된 구조 (참고용)
- [003-2-simplify-prediction-heads](003-2-simplify-prediction-heads.md) - 제거된 구조 (참고용)

---

## Implementation Results

### Changed Files
| File | Changes |
|------|---------|
| `src/prediction/prediction_model.py` | Added `BahdanauAttention` layer class with additive attention mechanism |
| `src/prediction/prediction_model.py` | Modified `create_model_lstm()` to accept `attention_type` parameter (None, 'bahdanau', 'multihead') |
| `src/prediction/prediction_model.py` | Added `attention_type` parameter to `ModelHandler.__init__()` |
| `src/prediction/prediction_model.py` | Updated handler functions (`training_handler`, `inference_handler`, `evaluation_handler`, `train_and_eval_handler`) to support attention_type |
| `src/prediction/prediction_model.py` | Added attention-related constants: `ATTENTION_TYPE`, `ATTENTION_UNITS`, `NUM_HEADS`, `KEY_DIM` |
| `src/prediction/prediction_model.py` | Added usage examples in `__main__` block for Bahdanau and Multi-Head Attention |

### Model Architecture Details
- **Standard LSTM**: 37,826 parameters
- **Bahdanau Attention**: 46,082 parameters (+8,256 from attention layer)
- **Multi-Head Attention**: 71,042 parameters (+33,216 from attention layers)

### Test Results
- All three model types create successfully
- Import and initialization work correctly
- Model summary shows proper layer connections

```
=== Test 1: Standard LSTM ===
Model name: lstm_gate
Total params: 37,826

=== Test 2: Bahdanau Attention ===
Model name: lstm_bahdanau_gate
Total params: 46,082

=== Test 3: Multi-Head Attention ===
Model name: lstm_multihead_gate
Total params: 71,042
```

### Lessons Learned / Issues
- Keras 3.0에서 `tf.reduce_mean()` 같은 TensorFlow 함수를 KerasTensor에 직접 사용 불가
- `GlobalAveragePooling1D` 레이어로 대체하여 해결
- Attention weights 반환은 Bahdanau에서만 지원 (해석 가능성 분석용)

### Follow-up
- 실제 학습 실험 및 Baseline vs Attention 성능 비교 필요
- Attention weights 시각화 함수 구현 필요
- 하이퍼파라미터 튜닝 (attention_units, num_heads, key_dim)

### Completed
2025-12-24

