---
title: RAG
parent: 개발
nav_order: 23
description: "RAG(Retrieval-Augmented Generation)의 개요, 핵심 원리, 파이프라인 구조, 청킹·임베딩·리랭킹 기술, GraphRAG·Agentic RAG 등 발전 형태, LLM 성능 개선 원리, 주요 서비스 예시까지 상세 정리."
---

# RAG(Retrieval-Augmented Generation)
{:.no_toc}

LLM의 환각(Hallucination)과 지식 한계를 극복하기 위해 **외부 지식을 검색하여 생성에 활용**하는 기술. 2020년 Meta(Facebook AI Research)가 논문으로 제안한 이후, 엔터프라이즈 AI의 핵심 아키텍처로 자리잡았다.

## 목차
{:.no_toc}

1. TOC
{:toc}

---

## 개요

### RAG란 무엇인가

RAG(Retrieval-Augmented Generation, 검색 증강 생성)는 LLM이 응답을 생성할 때 **외부 지식 소스에서 관련 정보를 검색(Retrieve)하여 프롬프트에 추가(Augment)한 뒤 답변을 생성(Generate)**하는 프레임워크다.

```
사용자 질문 → [검색(Retrieval)] → 관련 문서 추출
                                      ↓
              [증강(Augmentation)] → 프롬프트에 컨텍스트 추가
                                      ↓
              [생성(Generation)]  → LLM이 근거 기반 답변 생성
```

핵심 아이디어는 단순하다. LLM이 모든 지식을 파라미터에 저장하는 대신, **필요한 시점에 관련 정보를 외부에서 가져와 참조**하게 하는 것이다. 이는 사람이 시험을 볼 때 모든 것을 암기하는 대신 참고자료를 펼쳐놓고 답안을 작성하는 것과 같다.

### 왜 RAG가 필요한가

LLM은 강력하지만, 근본적인 한계가 존재한다.

| 문제 | 설명 | RAG의 해결 방식 |
|------|------|----------------|
| **환각(Hallucination)** | 사실이 아닌 내용을 자신 있게 생성 | 검색된 문서를 근거로 답변, 출처 명시 |
| **지식 단절(Knowledge Cutoff)** | 학습 데이터 이후의 정보를 모름 | 실시간 데이터소스에서 최신 정보 검색 |
| **도메인 지식 부족** | 기업 내부 데이터, 전문 분야에 약함 | 조직 고유 문서를 지식 소스로 연결 |
| **검증 불가능** | 답변의 근거를 확인할 수 없음 | 인용(citation)과 출처 제공 |

### RAG vs Fine-tuning

LLM의 성능을 개선하는 두 가지 주요 접근법을 비교하면 다음과 같다.

| 기준 | RAG | Fine-tuning |
|------|-----|-------------|
| **원리** | 외부 지식을 검색하여 프롬프트에 추가 | 모델 파라미터를 도메인 데이터로 재학습 |
| **모델 변경** | 모델 수정 없음 | 모델 가중치(weight) 조정 |
| **데이터 업데이트** | 문서 인덱스만 갱신하면 됨 | 재학습 필요 (비용·시간 소요) |
| **최신성** | 실시간 데이터 반영 가능 | 학습 시점 데이터에 고정 |
| **비용** | 인프라 비용 (벡터 DB, 검색 등) | GPU 학습 비용 |
| **추론 속도** | 검색 단계로 인한 지연 발생 | 추가 지연 없음 |
| **적합한 경우** | 동적·대규모 지식 기반, 최신 정보 필요 | 특정 도메인의 톤·스타일·전문 용어 학습 |

실무에서는 두 기법을 결합한 **하이브리드 접근**도 활발하다. Fine-tuning으로 도메인 톤과 추론 패턴을 학습시킨 뒤, RAG로 사실적 근거를 제공하는 방식이다.

---

## 핵심 파이프라인

RAG 시스템은 크게 **인덱싱(Indexing)**과 **검색-생성(Retrieval-Generation)** 두 단계로 나뉜다.

### 1단계: 인덱싱 (Indexing)

외부 문서를 검색 가능한 형태로 사전 처리하는 과정.

```
원본 문서 → [파싱] → [청킹] → [임베딩] → 벡터 DB 저장
  (PDF, HTML,       텍스트 추출   의미 단위    숫자 벡터로     인덱스 구축
   DB, API 등)                    분할        변환
```

#### 문서 파싱 (Document Parsing)

다양한 형식의 원본 데이터에서 텍스트를 추출하는 첫 단계.

- **PDF**: 텍스트 추출, OCR (스캔 문서), 테이블 구조 보존
- **HTML**: 태그 제거, 본문 추출, 메타데이터 보존
- **데이터베이스**: 쿼리 결과를 텍스트로 변환
- **API**: 실시간 데이터를 구조화된 텍스트로 변환

#### 청킹 (Chunking)

긴 문서를 **검색과 처리에 적합한 크기의 단위**로 분할하는 핵심 과정. 청킹 전략은 RAG 성능에 직접적인 영향을 미친다.

| 전략 | 방식 | 장점 | 단점 |
|------|------|------|------|
| **고정 크기** | 일정 토큰/문자 수로 분할 | 구현 간단, 비용 낮음 | 의미 단위가 끊길 수 있음 |
| **문장/단락 기반** | 문장·단락 경계에서 분할 | 자연스러운 의미 보존 | 청크 크기 불균일 |
| **시맨틱 청킹** | 임베딩 유사도로 의미 변화 지점 감지 | 의미적 완결성 높음 | API 호출 비용 발생 |
| **계층적 청킹** | 문서 → 섹션 → 단락 구조 보존 | 컨텍스트 유지, 멀티 해상도 검색 | 구현 복잡도 높음 |
| **명제 기반** | LLM이 원자적 주장(claim) 단위로 추출 | 정밀한 사실 검색 | LLM 호출 비용 높음 |

최근 연구에서는 **400토큰 청크 + text-embedding-3-large** 조합이 88~89% 재현율(recall)로 안정적인 성능을 보인다는 결과가 있다.

#### 임베딩 (Embedding)

텍스트 청크를 **고차원 벡터 공간의 수치 표현**으로 변환하는 과정. 의미적으로 유사한 텍스트는 벡터 공간에서 가까운 위치에 배치된다.

```
"비트코인 가격이 상승했다"  →  [0.23, -0.15, 0.87, ..., 0.42]  (1536차원)
"BTC 시세가 올랐다"        →  [0.21, -0.14, 0.85, ..., 0.40]  (유사한 벡터)
"오늘 날씨가 좋다"          →  [-0.65, 0.32, -0.11, ..., 0.78]  (먼 벡터)
```

주요 임베딩 모델:

| 모델 | 제공사 | 차원 | 특징 |
|------|--------|------|------|
| text-embedding-3-large | OpenAI | 3072 | 높은 범용 성능 |
| text-embedding-3-small | OpenAI | 1536 | 비용 효율적 |
| Cohere Embed v3 | Cohere | 1024 | 다국어 지원 우수 |
| BGE-M3 | BAAI | 1024 | 오픈소스, 다국어 |
| Voyage 3 | Voyage AI | 1024 | 코드 검색에 강점 |

#### 벡터 데이터베이스 (Vector Database)

임베딩 벡터를 저장하고 **유사도 기반 검색(Similarity Search)**을 수행하는 전용 데이터베이스.

주요 벡터 DB:
- **Pinecone**: 완전 관리형, 엔터프라이즈급 확장성
- **Weaviate**: 오픈소스, 하이브리드 검색 지원
- **Chroma**: 경량, 로컬 개발에 적합
- **Milvus**: 오픈소스, 대규모 데이터 처리
- **pgvector**: PostgreSQL 확장, 기존 DB 인프라 활용

### 2단계: 검색 (Retrieval)

사용자 질의를 벡터로 변환하고, 인덱싱된 문서 청크들 중 **가장 관련성 높은 것들을 검색**하는 과정.

#### 검색 방식

| 방식 | 원리 | 강점 | 약점 |
|------|------|------|------|
| **Dense Retrieval** | 쿼리와 문서를 동일 임베딩 공간에서 코사인 유사도로 비교 | 의미적 유사성 포착 | 정확한 키워드 매칭 약함 |
| **Sparse Retrieval** | TF-IDF, BM25 등 키워드 빈도 기반 매칭 | 정확한 용어 매칭 | 동의어·의역 처리 약함 |
| **하이브리드 검색** | Dense + Sparse 결과를 결합 | 두 방식의 장점 통합 | 가중치 튜닝 필요 |

하이브리드 검색에서는 **RRF(Reciprocal Rank Fusion)** 같은 알고리즘으로 두 검색 결과의 순위를 통합한다.

#### 리랭킹 (Reranking)

초기 검색 결과를 **더 정밀한 모델로 재평가하여 순위를 재조정**하는 과정. 검색의 정확도를 크게 높이는 핵심 기술이다.

```
초기 검색 (Top-50)  →  리랭커 모델  →  최종 결과 (Top-5)
  빠르지만 거친 검색      정밀 재평가       가장 관련성 높은 문서만
```

- **Cross-Encoder 방식**: 쿼리-문서 쌍을 동시에 분석하여 관련도 점수 산출
- 최신 모델(zerank-1 등)은 NDCG@10 기준 +28% 개선을 달성
- "Lost in the Middle" 문제(중간에 위치한 관련 정보를 LLM이 간과) 해결에 효과적

### 3단계: 증강 및 생성 (Augmentation & Generation)

검색된 문서를 **프롬프트에 컨텍스트로 주입**하고, LLM이 이를 참조하여 답변을 생성한다.

```python
# 증강된 프롬프트 구성 (개념적 예시)
prompt = f"""
다음 참고 자료를 바탕으로 질문에 답변하세요.
참고 자료에 없는 내용은 "확인할 수 없다"고 답변하세요.

[참고 자료]
{retrieved_documents}

[질문]
{user_query}

[답변]
"""
response = llm.generate(prompt)
```

이 단계에서 핵심은:
- **프롬프트 엔지니어링**: 검색 결과를 효과적으로 활용하도록 지시문 설계
- **컨텍스트 윈도우 관리**: 검색 결과가 너무 많으면 토큰 한도 초과 → 리랭킹으로 상위 결과만 선별
- **출처 명시**: 답변에 참조한 문서의 출처를 포함하여 검증 가능성 확보

---

## 발전 형태

RAG는 초기의 단순한 검색-생성 파이프라인에서 다양한 방향으로 진화하고 있다.

### Naive RAG → Advanced RAG → Modular RAG

RAG의 발전 단계를 3세대로 구분할 수 있다.

**1세대: Naive RAG**
- 기본적인 검색 → 생성 파이프라인
- 단순 벡터 유사도 검색
- 한계: 검색 품질 의존, 노이즈에 취약

**2세대: Advanced RAG**
- 사전/사후 검색 최적화 도입
- 쿼리 재작성(Query Rewriting), 리랭킹, 청크 최적화
- HyDE(Hypothetical Document Embeddings): LLM이 가상 답변을 먼저 생성하고, 그 답변으로 검색

**3세대: Modular RAG**
- 각 구성 요소를 독립 모듈로 분리
- 필요에 따라 모듈을 조합·교체 가능
- 라우팅, 스케줄링, 융합(fusion) 등 고급 기능

### GraphRAG

**지식 그래프(Knowledge Graph)를 검색 구조로 활용**하는 RAG. 기존 벡터 검색의 한계를 관계(relationship) 기반 탐색으로 보완한다.

```
[기존 RAG]
질문 → 벡터 검색 → 유사한 청크들 → LLM 생성

[GraphRAG]
질문 → 벡터 검색 + 그래프 탐색 → 엔티티 간 관계 파악 → LLM 생성
```

핵심 구성 요소:
- **온톨로지(Ontology)**: 도메인의 개념과 관계를 형식적으로 정의한 체계
- **지식 그래프**: 온톨로지 기반으로 실제 데이터를 **트리플(주어-술어-목적어)** 형태로 저장
  - 예: `(비트코인, 하위분류, 암호화폐)`, `(이더리움, 사용, 스마트계약)`
- **그래프 DB**: Neo4j, Amazon Neptune 등 그래프 구조 전용 데이터베이스

GraphRAG의 강점:
- 개체 간 **다단계 관계(multi-hop reasoning)** 추론 가능
- 검색 정밀도를 최대 99%까지 향상시킨 사례
- 과학 문헌, 금융 데이터, 제조업 등 복잡한 관계가 중요한 도메인에 적합

### Agentic RAG

**AI 에이전트가 검색 과정을 자율적으로 제어**하는 RAG. 단일 검색이 아닌, 에이전트가 반복적으로 판단·검색·검증하는 루프를 수행한다.

```
사용자 질문
    ↓
[에이전트] → 질문 분석 → 검색 전략 수립
    ↓
[검색 실행] → 결과 평가 → 충분한가?
    ↓                        ↑ (불충분하면 재검색)
   예 →  [답변 생성] → 품질 검증 → 최종 응답
```

기존 RAG와의 차이:

| 기존 RAG | Agentic RAG |
|----------|-------------|
| 고정된 단일 검색 | 동적, 반복적 검색 |
| 하나의 데이터 소스 | 복수의 도구·API 활용 |
| 검색 결과를 그대로 사용 | 결과를 평가하고 보완 |
| 단순 파이프라인 | 계획 → 실행 → 검증 루프 |

Agentic RAG는 복잡한 질문에 대해 **하위 질문으로 분해하고, 각각 최적의 소스에서 검색한 뒤, 결과를 종합**하는 고급 추론을 수행한다.

### Self-RAG 및 Corrective RAG

검색 결과의 **품질을 스스로 평가하고 수정**하는 자기 교정형 RAG.

- **Self-RAG**: 검색이 필요한지 자체 판단 → 검색 결과의 관련성 평가 → 불필요한 정보 필터링
- **Corrective RAG (CRAG)**: 검색 결과의 신뢰도를 평가하여, 낮은 경우 웹 검색 등 대체 소스로 보완

### Cache-Augmented Generation (CAG)

**자주 질의되는 지식을 KV 캐시에 사전 로드**하여 검색 단계를 생략하는 방식. 기존 RAG 대비 최대 40.5배 빠른 응답 속도를 달성한다. 정적이고 반복적인 질의 패턴에 적합하다.

---

## LLM과의 상호작용 원리

### RAG가 LLM 성능을 개선하는 메커니즘

RAG는 LLM의 생성 능력은 그대로 유지하면서, **입력 단계에서 근거 정보를 주입**하여 출력 품질을 높인다.

```
[Without RAG]
LLM(질문) → 파라미터 지식만으로 답변 → 환각 위험

[With RAG]
LLM(질문 + 검색된 컨텍스트) → 근거 기반 답변 → 정확도 향상
```

구체적인 개선 효과:

1. **사실적 정확성 (Factual Accuracy)**
   - 검색된 문서가 "근거(grounding)"로 작용하여 환각 감소
   - 출처를 함께 제공하여 답변의 검증 가능성 확보

2. **지식의 최신성 (Recency)**
   - 모델 재학습 없이 인덱스 갱신만으로 최신 정보 반영
   - 뉴스, 시장 데이터, 법규 변경 등 동적 정보에 대응

3. **도메인 특화 (Domain Specificity)**
   - 기업 내부 문서, 사내 규정 등을 지식 소스로 연결
   - 학습 데이터에 없는 전문 지식 활용 가능

4. **제어 가능성 (Controllability)**
   - 어떤 문서를 참조할지 제어 가능
   - 접근 권한(RBAC)에 따른 정보 필터링
   - 답변 근거의 감사(audit) 추적 가능

### 프롬프트 컨텍스트 윈도우와 RAG

LLM의 컨텍스트 윈도우 크기가 확대(4K → 128K → 1M+ 토큰)되면서 RAG의 역할에 대한 논의가 있지만, RAG는 여전히 유효하다.

| 관점 | 긴 컨텍스트만 사용 | RAG + 적절한 컨텍스트 |
|------|-------------------|---------------------|
| 비용 | 대량 토큰 = 높은 비용 | 필요한 정보만 검색 = 비용 효율 |
| 정확도 | "Lost in the Middle" 문제 | 리랭킹으로 핵심 정보 상위 배치 |
| 속도 | 긴 입력 = 느린 추론 | 짧은 컨텍스트 = 빠른 응답 |
| 확장성 | 전체 문서를 매번 입력 불가능 | 수백만 문서에서도 효율적 검색 |

---

## 주요 서비스 및 적용 사례

### 소비자 서비스

| 서비스 | RAG 활용 방식 |
|--------|--------------|
| **Perplexity AI** | 실시간 웹 검색 + LLM 생성으로 출처 기반 답변 제공. 2025년 기준 월 7.8억 쿼리 처리 |
| **ChatGPT (웹 브라우징)** | Bing 검색 결과를 RAG 방식으로 활용하여 최신 정보 답변 |
| **Google Gemini** | Google 검색 인프라와 결합한 Grounding 기능 |
| **Microsoft Copilot** | Bing 검색 + Microsoft Graph 데이터로 업무 맥락 결합 |

### 엔터프라이즈 솔루션

| 솔루션 | 특징 |
|--------|------|
| **Amazon Bedrock Knowledge Bases** | AWS 인프라에서 관리형 RAG 파이프라인 구축 |
| **Azure AI Search + OpenAI** | 기업 문서 위에 GPT 기반 질의응답 구축 |
| **LangChain / LlamaIndex** | RAG 파이프라인 구축을 위한 오픈소스 프레임워크 |
| **Glean** | 기업 내부 검색에 특화된 AI 어시스턴트 |

### 산업별 적용 사례

- **금융**: 실시간 시장 데이터·리서치 리포트를 검색하여 투자 분석 보고서 생성
- **의료**: 의사의 진료 기록·의학 문헌을 참조하여 임상 의사결정 지원
- **법률**: 판례·법령 DB를 검색하여 법률 자문 초안 작성
- **제조**: 설비 매뉴얼·정비 기록을 활용한 고장 진단 및 대응 가이드
- **고객 지원**: 제품 문서·FAQ를 기반으로 정확한 고객 응대 자동화

---

## RAG 시스템 구축 시 고려사항

### 성능 최적화 체크리스트

| 단계 | 최적화 포인트 |
|------|-------------|
| **청킹** | 도메인에 맞는 청크 크기·전략 실험 (400~500 토큰이 일반적 출발점) |
| **임베딩** | 도메인 특화 벤치마크로 모델 선정, 다국어 지원 확인 |
| **검색** | 하이브리드 검색(Dense + Sparse) 적용, 메타데이터 필터링 추가 |
| **리랭킹** | Cross-Encoder 기반 리랭커로 검색 정밀도 향상 |
| **프롬프트** | 검색 결과 활용 지시문 최적화, 출처 명시 유도 |
| **평가** | 검색 재현율(Recall), 정밀도(Precision), 답변 충실도(Faithfulness) 측정 |

### 평가 지표

RAG 시스템의 성능을 측정하는 주요 지표는 다음과 같다.

- **검색 품질**: Recall@K (상위 K개에 정답 포함 비율), MRR (Mean Reciprocal Rank)
- **생성 품질**: Faithfulness (검색 문서에 근거한 답변 비율), Answer Relevancy (질문에 대한 답변 적합도)
- **종합 평가**: RAGAS, TruLens 등 RAG 전용 평가 프레임워크 활용

---

## 정리

| 구분 | 내용 |
|------|------|
| **정의** | 외부 지식을 검색하여 LLM 생성에 활용하는 프레임워크 |
| **핵심 파이프라인** | 인덱싱(파싱→청킹→임베딩→저장) → 검색 → 증강 → 생성 |
| **주요 기술** | 하이브리드 검색, 리랭킹, 시맨틱 청킹, Cross-Encoder |
| **발전 형태** | GraphRAG, Agentic RAG, Self-RAG, Corrective RAG, CAG |
| **LLM 개선 효과** | 환각 감소, 최신 정보 반영, 도메인 특화, 출처 제공 |
| **적용 분야** | 검색 엔진(Perplexity), 엔터프라이즈 AI, 금융·의료·법률 등 |

RAG는 단순한 "검색 + 생성" 파이프라인에서 **자율적으로 판단하고 검색하는 에이전트형 아키텍처**로 진화하고 있으며, 엔터프라이즈 AI의 핵심 인프라로 자리잡고 있다.

---

### 참고 자료

- [RAG in 2026: Bridging Knowledge and Generative AI - Squirro](https://squirro.com/squirro-blog/state-of-rag-genai)
- [The Ultimate RAG Blueprint 2025/2026 - LangWatch](https://langwatch.ai/blog/the-ultimate-rag-blueprint-everything-you-need-to-know-about-rag-in-2025-2026)
- [What is RAG? - IBM](https://www.ibm.com/think/topics/retrieval-augmented-generation)
- [RAG란? - AWS](https://aws.amazon.com/what-is/retrieval-augmented-generation/)
- [RAG의 한계를 Knowledge Graph로 극복하기 - SOTAAZ](https://blog.sotaaz.com/post/ontology-knowledge-graph-rag)
- [Advanced RAG Techniques - Neo4j](https://neo4j.com/blog/genai/advanced-rag-techniques/)
- [RAG vs Fine-tuning - IBM](https://www.ibm.com/think/topics/rag-vs-fine-tuning)
- [검색 증강 생성 안내서 - Elastic](https://www.elastic.co/kr/what-is/retrieval-augmented-generation)
